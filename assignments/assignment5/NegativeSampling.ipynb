{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J5hOSYYYwNb"
      },
      "source": [
        "# Задание 5.2 - Word2Vec with Negative Sampling\n",
        "\n",
        "В этом задании мы натренируем свои версию word vectors с negative sampling на том же небольшом датасете.\n",
        "\n",
        "\n",
        "Несмотря на то, что основная причина использования Negative Sampling - улучшение скорости тренировки word2vec, в нашем игрушечном примере мы **не требуем** улучшения производительности. Мы используем negative sampling просто как дополнительное упражнение для знакомства с PyTorch.\n",
        "\n",
        "Перед запуском нужно запустить скрипт `download_data.sh`, чтобы скачать данные.\n",
        "\n",
        "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arzwJ1xBYwNh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
        "# so make sure you install dependencies from requirements.txt!\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijDIoEXIY9Ta",
        "outputId": "0b1a7cd4-5090-46dd-a419-6d5b02c2cac1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
        "!unzip stanfordSentimentTreebank.zip\n",
        "!rm stanfordSentimentTreebank.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guj1Em0aZHjd",
        "outputId": "a68d9606-8fa0-4f7d-af57-9a1b0b6eea3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-02 16:16:53--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cs.stanford.edu/srcf_404 [following]\n",
            "--2023-07-02 16:16:53--  https://cs.stanford.edu/srcf_404\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘stanfordSentimentTreebank.zip’\n",
            "\n",
            "stanfordSentimentTr     [ <=>                ]  39.77K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-07-02 16:16:54 (804 KB/s) - ‘stanfordSentimentTreebank.zip’ saved [40722]\n",
            "\n",
            "Archive:  stanfordSentimentTreebank.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of stanfordSentimentTreebank.zip or\n",
            "        stanfordSentimentTreebank.zip.zip, and cannot find stanfordSentimentTreebank.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbFw--fDYwNk",
        "outputId": "bc5b8230-72dc-4c3b-eec1-a3847ae5d6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num tokens: 19538\n",
            "literary ['detective', 'aficionados']\n",
            "it [\"'70s\", 'starred']\n",
            "cunning ['leonine', 'full-bodied', 'aging', 'sandeman']\n",
            "justify ['really', 'three']\n",
            "colonics ['riffs', 'diciness', 'versus', 'ads']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class StanfordTreeBank:\n",
        "    '''\n",
        "    Wrapper for accessing Stanford Tree Bank Dataset\n",
        "    https://nlp.stanford.edu/sentiment/treebank.html\n",
        "\n",
        "    Parses dataset, gives each token and index and provides lookups\n",
        "    from string token to index and back\n",
        "\n",
        "    Allows to generate random context with sampling strategy described in\n",
        "    word2vec paper:\n",
        "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.index_by_token = {} # map of string -> token index\n",
        "        self.token_by_index = []\n",
        "\n",
        "        self.sentences = []\n",
        "\n",
        "        self.token_freq = {}\n",
        "\n",
        "        self.token_reject_by_index = None\n",
        "\n",
        "    def load_dataset(self, folder):\n",
        "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
        "\n",
        "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
        "            l = f.readline() # skip the first line\n",
        "\n",
        "            for l in f:\n",
        "                splitted_line = l.strip().split()  # удаляем лишние пробелы и разделяем по пробелам\n",
        "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
        "                # сохраняем слова из списка в нижнем регистре\n",
        "\n",
        "                # сохраняем список слов\n",
        "                self.sentences.append(words)\n",
        "                for word in words:\n",
        "                    if word in self.token_freq:\n",
        "                        self.token_freq[word] +=1\n",
        "                    else:\n",
        "                        index = len(self.token_by_index)  # берем длину словаря\n",
        "                        self.token_freq[word] = 1  # добавялем новый элемент\n",
        "                        self.index_by_token[word] = index  # сохраняем индекс этого элемента -\n",
        "                        # длина словара увеличилась, но индекс = новая_длина - 1 = старая длина\n",
        "                        self.token_by_index.append(word)  # сохраняем в список слово.\n",
        "        self.compute_token_prob()\n",
        "\n",
        "    def compute_token_prob(self):\n",
        "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
        "        words_freq = words_count / np.sum(words_count)\n",
        "\n",
        "        # Following sampling strategy from word2vec paper\n",
        "        self.token_reject_by_index = 1 - np.sqrt(1e-5/words_freq)\n",
        "\n",
        "    def check_reject(self, word):\n",
        "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
        "\n",
        "    def get_random_context(self, context_length=5):\n",
        "        \"\"\"\n",
        "        Returns tuple of center word and list of context words\n",
        "        \"\"\"\n",
        "        sentence_sampled = []\n",
        "        while len(sentence_sampled) <= 2:\n",
        "            sentence_index = np.random.randint(len(self.sentences))\n",
        "            sentence = self.sentences[sentence_index]\n",
        "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
        "\n",
        "        center_word_index = np.random.randint(len(sentence_sampled))\n",
        "\n",
        "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
        "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
        "\n",
        "        return sentence_sampled[center_word_index], words_before+words_after\n",
        "\n",
        "    def num_tokens(self):\n",
        "        return len(self.token_by_index)\n",
        "\n",
        "data = StanfordTreeBank()\n",
        "data.load_dataset(\"/content/drive/MyDrive/Colab Notebooks/DL/stanfordSentimentTreebank/\")\n",
        "\n",
        "print(\"Num tokens:\", data.num_tokens())\n",
        "for i in range(5):\n",
        "    center_word, other_words = data.get_random_context(5)\n",
        "    print(center_word, other_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-ryfeAvZiM8",
        "outputId": "6e1ead5f-c700-4a98-d5a3-8b09a688b27e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR5AjJkrYwNm"
      },
      "source": [
        "# Dataset для Negative Sampling должен быть немного другим\n",
        "\n",
        "Как и прежде, Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
        "\n",
        "Здесь мы реализуем прямой проход модели сами, поэтому выдавать данные можно в удобном нам виде.\n",
        "Напоминаем, что в случае negative sampling каждым сэмплом является:\n",
        "- вход: слово в one-hot представлении\n",
        "- выход: набор из одного целевого слова и K других случайных слов из словаря.\n",
        "Вместо softmax + cross-entropy loss, сеть обучается через binary cross-entropy loss - то есть, предсказывает набор бинарных переменных, для каждой из которых функция ошибки считается независимо.\n",
        "\n",
        "Для целевого слова бинарное предсказание должно быть позитивным, а для K случайных слов - негативным.\n",
        "\n",
        "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте), в каждом из них K+1 целевых слов, для только одного из которых предсказание должно быть позитивным.\n",
        "Например, для K=2:\n",
        "\n",
        "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
        "- input: `orders`, target: `[love: 1, any: 0, rose: 0]`\n",
        "- input: `orders`, target: `[nicest: 1, fool: 0, grass: 0]`\n",
        "- input: `orders`, target: `[to: 1, -: 0, the: 0]`\n",
        "- input: `orders`, target: `[50-year: 1, ?: 0, door: 0]`\n",
        "\n",
        "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['1'] + [f'{i}' for i in range(2, 5, 1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdc7md7dketW",
        "outputId": "ba4a3d16-72f5-4393-978b-25ef909394e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '2', '3', '4']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBt4Jr6PYwNn",
        "outputId": "5b932739-c516-4216-ba54-38f9ef6a650c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample - input: 10236, output indices: tensor([ 4974,  4180,  8000, 18540,  7795, 14376, 16183,  1639,  9416, 13908,\n",
            "        17142]), output target: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "num_negative_samples = 10\n",
        "\n",
        "class Word2VecNegativeSampling(Dataset):\n",
        "    '''\n",
        "    PyTorch Dataset for Word2Vec with Negative Sampling.\n",
        "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
        "    a number of random contexts\n",
        "    '''\n",
        "    def __init__(self, data, num_negative_samples, num_contexts=30000):\n",
        "        '''\n",
        "        Initializes Word2VecNegativeSampling, but doesn't generate the samples yet\n",
        "        (for that, use generate_dataset)\n",
        "        Arguments:\n",
        "        data - StanfordTreebank instace\n",
        "        num_negative_samples - number of negative samples to generate in addition to a positive one\n",
        "        num_contexts - number of random contexts to use when generating a dataset\n",
        "        '''\n",
        "        self.samples = []\n",
        "        self.data = data\n",
        "        self.neg_samples_num = num_negative_samples\n",
        "        self.context_num = num_contexts\n",
        "\n",
        "        # TODO: Implement what you need for other methods!\n",
        "\n",
        "    def generate_dataset(self):\n",
        "        '''\n",
        "        Generates dataset samples from random contexts\n",
        "        Note: there will be more samples than contexts because every context\n",
        "        can generate more than one sample\n",
        "        '''\n",
        "        # TODO: Implement generating the dataset\n",
        "        # You should sample num_contexts contexts from the data and turn them into samples\n",
        "        # Note you will have several samples from one context\n",
        "        samples_arr = []\n",
        "\n",
        "        for i in range(self.context_num):\n",
        "            main_word, context_words = self.data.get_random_context()\n",
        "            # one_hot_vector = np.zeros(self.data.num_tokens())\n",
        "            # one_hot_vector[self.data.index_by_token[main_word]] = 1\n",
        "\n",
        "            for word in context_words:\n",
        "                # neg_tokens = []\n",
        "                neg_indexes = []\n",
        "                while len(neg_indexes) < self.neg_samples_num:\n",
        "                # for negative_word in self.neg_samples_num:\n",
        "                    neg_index = np.random.randint(self.data.num_tokens())\n",
        "                    neg_sample = self.data.token_by_index[neg_index]\n",
        "                    if neg_sample not in context_words:\n",
        "                        # neg_tokens.append(neg_sample)  # если сохранять слово\n",
        "                        neg_indexes.append(neg_index)  # если сохранять индекс слова\n",
        "\n",
        "                # samples_arr.append((one_hot_vector, [word] + neg_samples))  # передаем список слов\n",
        "                samples_arr.append((self.data.index_by_token[center_word], [self.data.index_by_token[word]] + neg_indexes))  # передаем список индексов слов\n",
        "\n",
        "        self.samples = samples_arr\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns total number of samples\n",
        "        '''\n",
        "        # TODO: Return the number of samples\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Returns i-th sample\n",
        "        Return values:\n",
        "        input_vector - index of the input word (not torch.Tensor!)\n",
        "        output_indices - torch.Tensor of indices of the target words. Should be 1+num_negative_samples.\n",
        "        output_target - torch.Tensor with float targets for the training. Should be the same size as output_indices\n",
        "                        and have 1 for the context word and 0 everywhere else\n",
        "        '''\n",
        "        # TODO: Generate tuple of 3 return arguments for i-th sample\n",
        "\n",
        "        one_hot_vector, context_indexes = self.samples[index]\n",
        "        output_target = np.zeros(len(context_indexes))\n",
        "        output_target[0] = 1\n",
        "\n",
        "        return one_hot_vector, torch.tensor(context_indexes), torch.tensor(output_target)\n",
        "\n",
        "dataset = Word2VecNegativeSampling(data, num_negative_samples, 10)\n",
        "dataset.generate_dataset()\n",
        "input_vector, output_indices, output_target = dataset[0]\n",
        "\n",
        "print(\"Sample - input: %s, output indices: %s, output target: %s\" % (int(input_vector), output_indices, output_target)) # target should be able to convert to int\n",
        "assert isinstance(output_indices, torch.Tensor)\n",
        "assert output_indices.shape[0] == num_negative_samples+1\n",
        "\n",
        "assert isinstance(output_target, torch.Tensor)\n",
        "assert output_target.shape[0] == num_negative_samples+1\n",
        "assert torch.sum(output_target) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lpp43P8YwNo"
      },
      "source": [
        "# Создаем модель\n",
        "\n",
        "Для нашей задачи нам придется реализовать свою собственную PyTorch модель.\n",
        "Эта модель реализует свой собственный прямой проход (forward pass), который получает на вход индекс входного слова и набор индексов для выходных слов.\n",
        "\n",
        "Как всегда, на вход приходит не один сэмпл, а целый batch.  \n",
        "Напомним, что цели улучшить скорость тренировки у нас нет, достаточно чтобы она сходилась."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQkGNAL3YwNp"
      },
      "outputs": [],
      "source": [
        "# Create the usual PyTorch structures\n",
        "dataset = Word2VecNegativeSampling(data, num_negative_samples, 30000)\n",
        "dataset.generate_dataset()\n",
        "\n",
        "# As before, we'll be training very small word vectors!\n",
        "wordvec_dim = 10\n",
        "\n",
        "class Word2VecNegativeSamples(nn.Module):\n",
        "    def __init__(self, num_tokens):\n",
        "        super(Word2VecNegativeSamples, self).__init__()\n",
        "        self.input = nn.Linear(num_tokens, 10, bias=False)\n",
        "        self.ouput = nn.Linear(10, num_tokens, bias=False)\n",
        "\n",
        "    def forward(self, input_index_batch, output_indices_batch):\n",
        "        '''\n",
        "        Implements forward pass with negative sampling\n",
        "\n",
        "        Arguments:\n",
        "        input_index_batch - Tensor of ints, shape: (batch_size, ), indices of input words in the batch\n",
        "        output_indices_batch - Tensor if ints, shape: (batch_size, num_negative_samples+1),\n",
        "                                indices of the target words for every sample\n",
        "\n",
        "        Returns:\n",
        "        predictions - Tensor of floats, shape: (batch_size, um_negative_samples+1)\n",
        "        '''\n",
        "        results = []\n",
        "\n",
        "        # TODO Implement forward pass\n",
        "        # Hint: You can use for loop to go over all samples on the batch,\n",
        "        # run every sample indivisually and then use\n",
        "        # torch.stack or torch.cat to produce the final result\n",
        "\n",
        "nn_model = Word2VecNegativeSamples(data.num_tokens())\n",
        "nn_model.type(torch.FloatTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_aOvIaoYwNq"
      },
      "outputs": [],
      "source": [
        "def extract_word_vectors(nn_model):\n",
        "    '''\n",
        "    Extracts word vectors from the model\n",
        "\n",
        "    Returns:\n",
        "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "    '''\n",
        "    # TODO: Implement extracting word vectors from param weights\n",
        "    # return tuple of input vectors and output vectos\n",
        "\n",
        "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
        "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCJYAx-gYwNq"
      },
      "outputs": [],
      "source": [
        "def train_neg_sample(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
        "    '''\n",
        "    Trains word2vec with negative samples on and regenerating dataset every epoch\n",
        "\n",
        "    Returns:\n",
        "    loss_history, train_history\n",
        "    '''\n",
        "    loss = nn.BCEWithLogitsLoss().type(torch.FloatTensor)\n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Enter train mode\n",
        "\n",
        "        dataset.generate_dataset()\n",
        "\n",
        "        # TODO: Implement training using negative samples\n",
        "        # You can estimate accuracy by comparing prediction values with 0\n",
        "        # And don't forget to step the scheduler!\n",
        "\n",
        "        print(\"Average loss: %f, Train accuracy: %f\" % (ave_loss, train_accuracy))\n",
        "\n",
        "    return loss_history, train_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnyvXO97YwNr"
      },
      "source": [
        "# Ну и наконец тренировка!\n",
        "\n",
        "Добейтесь значения ошибки меньше **0.25**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPw0dhcDYwNr"
      },
      "outputs": [],
      "source": [
        "# Finally, let's train the model!\n",
        "\n",
        "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-1, weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
        "\n",
        "loss_history, train_history = train_neg_sample(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdbtSXH7YwNs"
      },
      "outputs": [],
      "source": [
        "# Visualize training graphs\n",
        "plt.subplot(211)\n",
        "plt.plot(train_history)\n",
        "plt.subplot(212)\n",
        "plt.plot(loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8e_AJx4YwNs"
      },
      "source": [
        "# Визуализируем вектора для разного вида слов до и после тренировки\n",
        "\n",
        "Как и ранее, в случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftwZvrNoYwNs"
      },
      "outputs": [],
      "source": [
        "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
        "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "\n",
        "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
        "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
        "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
        "\n",
        "    # Helpful words form CS244D example\n",
        "    # http://cs224d.stanford.edu/assignment1/index.html\n",
        "    visualize_words = {'green': [\"the\", \"a\", \"an\"],\n",
        "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"],\n",
        "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\",\n",
        "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
        "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
        "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
        "                     }\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.suptitle(title)\n",
        "    for color, words in visualize_words.items():\n",
        "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
        "        for i, word in enumerate(words):\n",
        "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
        "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
        "\n",
        "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
        "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryi2NCJ2YwNs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}