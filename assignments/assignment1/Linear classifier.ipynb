{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float32) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int).reshape(1, -1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 906.875189\n",
      "Epoch 1, loss: 825.588913\n",
      "Epoch 2, loss: 902.880364\n",
      "Epoch 3, loss: 827.801506\n",
      "Epoch 4, loss: 941.855854\n",
      "Epoch 5, loss: 880.560729\n",
      "Epoch 6, loss: 943.684482\n",
      "Epoch 7, loss: 1026.816850\n",
      "Epoch 8, loss: 930.310760\n",
      "Epoch 9, loss: 824.881920\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21355117130>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 957.197835\n",
      "Epoch 1, loss: 762.080095\n",
      "Epoch 2, loss: 1014.447648\n",
      "Epoch 3, loss: 868.023872\n",
      "Epoch 4, loss: 981.566897\n",
      "Epoch 5, loss: 788.060215\n",
      "Epoch 6, loss: 729.204651\n",
      "Epoch 7, loss: 899.792240\n",
      "Epoch 8, loss: 837.301030\n",
      "Epoch 9, loss: 853.895671\n",
      "Epoch 10, loss: 841.146115\n",
      "Epoch 11, loss: 885.066120\n",
      "Epoch 12, loss: 965.587200\n",
      "Epoch 13, loss: 1018.777317\n",
      "Epoch 14, loss: 872.095020\n",
      "Epoch 15, loss: 794.061886\n",
      "Epoch 16, loss: 915.652934\n",
      "Epoch 17, loss: 771.005088\n",
      "Epoch 18, loss: 906.534056\n",
      "Epoch 19, loss: 833.908113\n",
      "Epoch 20, loss: 793.153324\n",
      "Epoch 21, loss: 948.240102\n",
      "Epoch 22, loss: 802.878428\n",
      "Epoch 23, loss: 837.346457\n",
      "Epoch 24, loss: 718.400796\n",
      "Epoch 25, loss: 927.599381\n",
      "Epoch 26, loss: 925.029793\n",
      "Epoch 27, loss: 872.038027\n",
      "Epoch 28, loss: 992.073712\n",
      "Epoch 29, loss: 899.116682\n",
      "Epoch 30, loss: 735.088842\n",
      "Epoch 31, loss: 1060.131067\n",
      "Epoch 32, loss: 883.230770\n",
      "Epoch 33, loss: 856.332460\n",
      "Epoch 34, loss: 904.385127\n",
      "Epoch 35, loss: 874.907995\n",
      "Epoch 36, loss: 817.926652\n",
      "Epoch 37, loss: 853.155881\n",
      "Epoch 38, loss: 975.510802\n",
      "Epoch 39, loss: 884.767123\n",
      "Epoch 40, loss: 749.250161\n",
      "Epoch 41, loss: 981.599977\n",
      "Epoch 42, loss: 725.637297\n",
      "Epoch 43, loss: 749.016136\n",
      "Epoch 44, loss: 973.517608\n",
      "Epoch 45, loss: 894.591700\n",
      "Epoch 46, loss: 874.477942\n",
      "Epoch 47, loss: 840.361884\n",
      "Epoch 48, loss: 877.537741\n",
      "Epoch 49, loss: 996.567273\n",
      "Epoch 50, loss: 809.715029\n",
      "Epoch 51, loss: 759.224894\n",
      "Epoch 52, loss: 924.808459\n",
      "Epoch 53, loss: 952.117624\n",
      "Epoch 54, loss: 765.001310\n",
      "Epoch 55, loss: 805.396440\n",
      "Epoch 56, loss: 939.351606\n",
      "Epoch 57, loss: 974.573139\n",
      "Epoch 58, loss: 735.055408\n",
      "Epoch 59, loss: 966.249744\n",
      "Epoch 60, loss: 819.036457\n",
      "Epoch 61, loss: 880.356495\n",
      "Epoch 62, loss: 892.311688\n",
      "Epoch 63, loss: 1129.210153\n",
      "Epoch 64, loss: 751.626461\n",
      "Epoch 65, loss: 1007.342341\n",
      "Epoch 66, loss: 797.765767\n",
      "Epoch 67, loss: 765.701504\n",
      "Epoch 68, loss: 835.835185\n",
      "Epoch 69, loss: 1133.520078\n",
      "Epoch 70, loss: 968.193128\n",
      "Epoch 71, loss: 908.431620\n",
      "Epoch 72, loss: 830.431978\n",
      "Epoch 73, loss: 781.210449\n",
      "Epoch 74, loss: 832.782682\n",
      "Epoch 75, loss: 898.693751\n",
      "Epoch 76, loss: 761.463675\n",
      "Epoch 77, loss: 1058.341000\n",
      "Epoch 78, loss: 911.203172\n",
      "Epoch 79, loss: 863.820920\n",
      "Epoch 80, loss: 982.434566\n",
      "Epoch 81, loss: 987.322663\n",
      "Epoch 82, loss: 939.102805\n",
      "Epoch 83, loss: 954.785399\n",
      "Epoch 84, loss: 1010.045169\n",
      "Epoch 85, loss: 967.898633\n",
      "Epoch 86, loss: 827.878488\n",
      "Epoch 87, loss: 760.364420\n",
      "Epoch 88, loss: 1069.913835\n",
      "Epoch 89, loss: 782.188037\n",
      "Epoch 90, loss: 891.383807\n",
      "Epoch 91, loss: 1016.787441\n",
      "Epoch 92, loss: 953.181683\n",
      "Epoch 93, loss: 854.291263\n",
      "Epoch 94, loss: 889.532608\n",
      "Epoch 95, loss: 982.234403\n",
      "Epoch 96, loss: 867.149033\n",
      "Epoch 97, loss: 1007.750633\n",
      "Epoch 98, loss: 742.398591\n",
      "Epoch 99, loss: 752.073471\n",
      "Accuracy after training for 100 epochs:  0.177\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1058.794767\n",
      "Epoch 1, loss: 921.823509\n",
      "Epoch 2, loss: 906.006988\n",
      "Epoch 3, loss: 935.559478\n",
      "Epoch 4, loss: 810.942937\n",
      "Epoch 5, loss: 728.951552\n",
      "Epoch 6, loss: 963.570926\n",
      "Epoch 7, loss: 834.896203\n",
      "Epoch 8, loss: 885.470464\n",
      "Epoch 9, loss: 824.065783\n",
      "Epoch 10, loss: 852.038229\n",
      "Epoch 11, loss: 865.104587\n",
      "Epoch 12, loss: 680.933200\n",
      "Epoch 13, loss: 922.541473\n",
      "Epoch 14, loss: 851.217759\n",
      "Epoch 15, loss: 640.426259\n",
      "Epoch 16, loss: 828.102927\n",
      "Epoch 17, loss: 732.902943\n",
      "Epoch 18, loss: 822.604280\n",
      "Epoch 19, loss: 760.913547\n",
      "Epoch 20, loss: 879.098526\n",
      "Epoch 21, loss: 790.867654\n",
      "Epoch 22, loss: 685.039623\n",
      "Epoch 23, loss: 864.984825\n",
      "Epoch 24, loss: 685.264088\n",
      "Epoch 25, loss: 886.633361\n",
      "Epoch 26, loss: 644.402048\n",
      "Epoch 27, loss: 771.837225\n",
      "Epoch 28, loss: 784.187721\n",
      "Epoch 29, loss: 912.971698\n",
      "Epoch 30, loss: 908.594390\n",
      "Epoch 31, loss: 866.471491\n",
      "Epoch 32, loss: 741.179540\n",
      "Epoch 33, loss: 920.949534\n",
      "Epoch 34, loss: 878.463702\n",
      "Epoch 35, loss: 818.673886\n",
      "Epoch 36, loss: 818.699198\n",
      "Epoch 37, loss: 934.195526\n",
      "Epoch 38, loss: 775.572147\n",
      "Epoch 39, loss: 814.103586\n",
      "Epoch 40, loss: 837.769022\n",
      "Epoch 41, loss: 693.274391\n",
      "Epoch 42, loss: 824.839901\n",
      "Epoch 43, loss: 858.594321\n",
      "Epoch 44, loss: 801.212780\n",
      "Epoch 45, loss: 693.265178\n",
      "Epoch 46, loss: 873.816912\n",
      "Epoch 47, loss: 874.045171\n",
      "Epoch 48, loss: 1006.085397\n",
      "Epoch 49, loss: 868.396894\n",
      "Epoch 50, loss: 755.613957\n",
      "Epoch 51, loss: 779.735523\n",
      "Epoch 52, loss: 719.695502\n",
      "Epoch 53, loss: 671.589601\n",
      "Epoch 54, loss: 775.359730\n",
      "Epoch 55, loss: 827.156675\n",
      "Epoch 56, loss: 751.417460\n",
      "Epoch 57, loss: 702.152135\n",
      "Epoch 58, loss: 866.845661\n",
      "Epoch 59, loss: 725.081958\n",
      "Epoch 60, loss: 857.513868\n",
      "Epoch 61, loss: 891.104064\n",
      "Epoch 62, loss: 742.769808\n",
      "Epoch 63, loss: 801.201100\n",
      "Epoch 64, loss: 706.125137\n",
      "Epoch 65, loss: 707.601782\n",
      "Epoch 66, loss: 804.123051\n",
      "Epoch 67, loss: 657.719672\n",
      "Epoch 68, loss: 715.279434\n",
      "Epoch 69, loss: 867.050443\n",
      "Epoch 70, loss: 703.151073\n",
      "Epoch 71, loss: 954.595160\n",
      "Epoch 72, loss: 808.460893\n",
      "Epoch 73, loss: 706.172828\n",
      "Epoch 74, loss: 849.994018\n",
      "Epoch 75, loss: 863.406248\n",
      "Epoch 76, loss: 673.488932\n",
      "Epoch 77, loss: 795.353847\n",
      "Epoch 78, loss: 708.090912\n",
      "Epoch 79, loss: 930.165753\n",
      "Epoch 80, loss: 841.451426\n",
      "Epoch 81, loss: 861.297341\n",
      "Epoch 82, loss: 795.642316\n",
      "Epoch 83, loss: 707.370012\n",
      "Epoch 84, loss: 666.181903\n",
      "Epoch 85, loss: 735.825884\n",
      "Epoch 86, loss: 925.965187\n",
      "Epoch 87, loss: 888.958917\n",
      "Epoch 88, loss: 1010.987369\n",
      "Epoch 89, loss: 722.177516\n",
      "Epoch 90, loss: 735.158554\n",
      "Epoch 91, loss: 740.862212\n",
      "Epoch 92, loss: 711.633242\n",
      "Epoch 93, loss: 832.300130\n",
      "Epoch 94, loss: 680.276797\n",
      "Epoch 95, loss: 765.823829\n",
      "Epoch 96, loss: 796.502175\n",
      "Epoch 97, loss: 757.102928\n",
      "Epoch 98, loss: 676.038505\n",
      "Epoch 99, loss: 714.352117\n",
      "Epoch 100, loss: 915.076547\n",
      "Epoch 101, loss: 673.283404\n",
      "Epoch 102, loss: 807.576290\n",
      "Epoch 103, loss: 754.250380\n",
      "Epoch 104, loss: 772.119016\n",
      "Epoch 105, loss: 788.264371\n",
      "Epoch 106, loss: 677.836254\n",
      "Epoch 107, loss: 840.245449\n",
      "Epoch 108, loss: 972.736596\n",
      "Epoch 109, loss: 689.169379\n",
      "Epoch 110, loss: 808.232810\n",
      "Epoch 111, loss: 667.651586\n",
      "Epoch 112, loss: 701.210693\n",
      "Epoch 113, loss: 891.027070\n",
      "Epoch 114, loss: 679.276130\n",
      "Epoch 115, loss: 665.184294\n",
      "Epoch 116, loss: 813.764714\n",
      "Epoch 117, loss: 949.705487\n",
      "Epoch 118, loss: 717.297690\n",
      "Epoch 119, loss: 715.901006\n",
      "Epoch 120, loss: 829.514705\n",
      "Epoch 121, loss: 717.994929\n",
      "Epoch 122, loss: 718.014565\n",
      "Epoch 123, loss: 901.597915\n",
      "Epoch 124, loss: 637.983307\n",
      "Epoch 125, loss: 799.010661\n",
      "Epoch 126, loss: 939.968012\n",
      "Epoch 127, loss: 996.498433\n",
      "Epoch 128, loss: 945.278027\n",
      "Epoch 129, loss: 813.348823\n",
      "Epoch 130, loss: 647.145648\n",
      "Epoch 131, loss: 723.421472\n",
      "Epoch 132, loss: 712.277786\n",
      "Epoch 133, loss: 718.809153\n",
      "Epoch 134, loss: 746.939556\n",
      "Epoch 135, loss: 786.575629\n",
      "Epoch 136, loss: 939.869462\n",
      "Epoch 137, loss: 680.848491\n",
      "Epoch 138, loss: 702.964913\n",
      "Epoch 139, loss: 759.239494\n",
      "Epoch 140, loss: 784.429628\n",
      "Epoch 141, loss: 677.600209\n",
      "Epoch 142, loss: 732.481295\n",
      "Epoch 143, loss: 807.348015\n",
      "Epoch 144, loss: 780.411217\n",
      "Epoch 145, loss: 745.183766\n",
      "Epoch 146, loss: 767.809453\n",
      "Epoch 147, loss: 833.447860\n",
      "Epoch 148, loss: 704.968563\n",
      "Epoch 149, loss: 659.245693\n",
      "Epoch 150, loss: 798.806068\n",
      "Epoch 151, loss: 677.082899\n",
      "Epoch 152, loss: 766.355721\n",
      "Epoch 153, loss: 793.972073\n",
      "Epoch 154, loss: 772.251420\n",
      "Epoch 155, loss: 868.620323\n",
      "Epoch 156, loss: 845.324708\n",
      "Epoch 157, loss: 705.509714\n",
      "Epoch 158, loss: 787.861237\n",
      "Epoch 159, loss: 820.309422\n",
      "Epoch 160, loss: 865.833015\n",
      "Epoch 161, loss: 777.957322\n",
      "Epoch 162, loss: 704.349701\n",
      "Epoch 163, loss: 721.235940\n",
      "Epoch 164, loss: 790.716381\n",
      "Epoch 165, loss: 778.256902\n",
      "Epoch 166, loss: 689.399102\n",
      "Epoch 167, loss: 737.449802\n",
      "Epoch 168, loss: 916.613601\n",
      "Epoch 169, loss: 781.548970\n",
      "Epoch 170, loss: 677.728903\n",
      "Epoch 171, loss: 687.657155\n",
      "Epoch 172, loss: 627.966288\n",
      "Epoch 173, loss: 771.075398\n",
      "Epoch 174, loss: 878.032472\n",
      "Epoch 175, loss: 691.847328\n",
      "Epoch 176, loss: 650.729658\n",
      "Epoch 177, loss: 715.262896\n",
      "Epoch 178, loss: 747.803545\n",
      "Epoch 179, loss: 812.850374\n",
      "Epoch 180, loss: 766.791733\n",
      "Epoch 181, loss: 893.886595\n",
      "Epoch 182, loss: 712.265842\n",
      "Epoch 183, loss: 692.650628\n",
      "Epoch 184, loss: 754.437612\n",
      "Epoch 185, loss: 691.959111\n",
      "Epoch 186, loss: 748.820800\n",
      "Epoch 187, loss: 812.599952\n",
      "Epoch 188, loss: 717.085930\n",
      "Epoch 189, loss: 650.812204\n",
      "Epoch 190, loss: 758.627926\n",
      "Epoch 191, loss: 739.618724\n",
      "Epoch 192, loss: 1067.321846\n",
      "Epoch 193, loss: 802.755293\n",
      "Epoch 194, loss: 752.808308\n",
      "Epoch 195, loss: 751.121416\n",
      "Epoch 196, loss: 837.947094\n",
      "Epoch 197, loss: 708.935282\n",
      "Epoch 198, loss: 859.525591\n",
      "Epoch 199, loss: 784.295998\n",
      "lr: 0.001 | reg: 0.0001 | accuracy: 0.153\n",
      "Epoch 0, loss: 768.909715\n",
      "Epoch 1, loss: 954.567551\n",
      "Epoch 2, loss: 763.113767\n",
      "Epoch 3, loss: 1002.950514\n",
      "Epoch 4, loss: 800.699828\n",
      "Epoch 5, loss: 810.005191\n",
      "Epoch 6, loss: 750.166835\n",
      "Epoch 7, loss: 973.089494\n",
      "Epoch 8, loss: 987.105901\n",
      "Epoch 9, loss: 830.854568\n",
      "Epoch 10, loss: 721.855247\n",
      "Epoch 11, loss: 842.274785\n",
      "Epoch 12, loss: 824.159328\n",
      "Epoch 13, loss: 709.526493\n",
      "Epoch 14, loss: 745.129940\n",
      "Epoch 15, loss: 710.775452\n",
      "Epoch 16, loss: 880.184017\n",
      "Epoch 17, loss: 764.992262\n",
      "Epoch 18, loss: 789.644260\n",
      "Epoch 19, loss: 740.401662\n",
      "Epoch 20, loss: 992.584275\n",
      "Epoch 21, loss: 866.346940\n",
      "Epoch 22, loss: 820.970547\n",
      "Epoch 23, loss: 859.369697\n",
      "Epoch 24, loss: 704.106179\n",
      "Epoch 25, loss: 826.750248\n",
      "Epoch 26, loss: 786.571704\n",
      "Epoch 27, loss: 843.740854\n",
      "Epoch 28, loss: 1032.309633\n",
      "Epoch 29, loss: 922.451993\n",
      "Epoch 30, loss: 823.435757\n",
      "Epoch 31, loss: 912.078881\n",
      "Epoch 32, loss: 1070.378534\n",
      "Epoch 33, loss: 726.110637\n",
      "Epoch 34, loss: 884.950881\n",
      "Epoch 35, loss: 838.943228\n",
      "Epoch 36, loss: 685.121758\n",
      "Epoch 37, loss: 785.497610\n",
      "Epoch 38, loss: 728.500521\n",
      "Epoch 39, loss: 691.746007\n",
      "Epoch 40, loss: 641.018440\n",
      "Epoch 41, loss: 903.984328\n",
      "Epoch 42, loss: 780.664471\n",
      "Epoch 43, loss: 759.132652\n",
      "Epoch 44, loss: 849.432257\n",
      "Epoch 45, loss: 712.763472\n",
      "Epoch 46, loss: 684.231915\n",
      "Epoch 47, loss: 791.115197\n",
      "Epoch 48, loss: 741.309294\n",
      "Epoch 49, loss: 805.619518\n",
      "Epoch 50, loss: 844.413596\n",
      "Epoch 51, loss: 883.380884\n",
      "Epoch 52, loss: 783.164994\n",
      "Epoch 53, loss: 801.216116\n",
      "Epoch 54, loss: 741.171391\n",
      "Epoch 55, loss: 772.454382\n",
      "Epoch 56, loss: 697.283528\n",
      "Epoch 57, loss: 769.356075\n",
      "Epoch 58, loss: 763.137434\n",
      "Epoch 59, loss: 788.528571\n",
      "Epoch 60, loss: 696.728401\n",
      "Epoch 61, loss: 652.022597\n",
      "Epoch 62, loss: 893.538048\n",
      "Epoch 63, loss: 681.902916\n",
      "Epoch 64, loss: 776.090308\n",
      "Epoch 65, loss: 818.519495\n",
      "Epoch 66, loss: 749.771437\n",
      "Epoch 67, loss: 706.516407\n",
      "Epoch 68, loss: 651.551754\n",
      "Epoch 69, loss: 705.305133\n",
      "Epoch 70, loss: 653.593055\n",
      "Epoch 71, loss: 854.364969\n",
      "Epoch 72, loss: 828.361639\n",
      "Epoch 73, loss: 749.013314\n",
      "Epoch 74, loss: 865.026867\n",
      "Epoch 75, loss: 743.508152\n",
      "Epoch 76, loss: 729.033127\n",
      "Epoch 77, loss: 675.488149\n",
      "Epoch 78, loss: 730.762041\n",
      "Epoch 79, loss: 719.960331\n",
      "Epoch 80, loss: 869.436574\n",
      "Epoch 81, loss: 882.805554\n",
      "Epoch 82, loss: 911.743600\n",
      "Epoch 83, loss: 711.835084\n",
      "Epoch 84, loss: 877.317634\n",
      "Epoch 85, loss: 967.829884\n",
      "Epoch 86, loss: 703.329411\n",
      "Epoch 87, loss: 884.397357\n",
      "Epoch 88, loss: 838.041743\n",
      "Epoch 89, loss: 684.622021\n",
      "Epoch 90, loss: 782.774094\n",
      "Epoch 91, loss: 682.239099\n",
      "Epoch 92, loss: 684.522895\n",
      "Epoch 93, loss: 723.687140\n",
      "Epoch 94, loss: 741.209688\n",
      "Epoch 95, loss: 788.379205\n",
      "Epoch 96, loss: 711.706576\n",
      "Epoch 97, loss: 810.732022\n",
      "Epoch 98, loss: 807.295568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 754.100706\n",
      "Epoch 100, loss: 691.164513\n",
      "Epoch 101, loss: 671.657028\n",
      "Epoch 102, loss: 679.817082\n",
      "Epoch 103, loss: 949.437784\n",
      "Epoch 104, loss: 693.663656\n",
      "Epoch 105, loss: 709.136036\n",
      "Epoch 106, loss: 818.129388\n",
      "Epoch 107, loss: 802.728968\n",
      "Epoch 108, loss: 739.152323\n",
      "Epoch 109, loss: 761.672204\n",
      "Epoch 110, loss: 757.013376\n",
      "Epoch 111, loss: 818.403858\n",
      "Epoch 112, loss: 743.981122\n",
      "Epoch 113, loss: 697.906146\n",
      "Epoch 114, loss: 816.806659\n",
      "Epoch 115, loss: 928.085947\n",
      "Epoch 116, loss: 680.825847\n",
      "Epoch 117, loss: 900.583463\n",
      "Epoch 118, loss: 776.361250\n",
      "Epoch 119, loss: 735.730278\n",
      "Epoch 120, loss: 893.835704\n",
      "Epoch 121, loss: 791.514177\n",
      "Epoch 122, loss: 823.971931\n",
      "Epoch 123, loss: 652.463151\n",
      "Epoch 124, loss: 647.717673\n",
      "Epoch 125, loss: 787.093279\n",
      "Epoch 126, loss: 685.219556\n",
      "Epoch 127, loss: 745.068225\n",
      "Epoch 128, loss: 638.901853\n",
      "Epoch 129, loss: 786.497724\n",
      "Epoch 130, loss: 774.976104\n",
      "Epoch 131, loss: 703.511259\n",
      "Epoch 132, loss: 852.000962\n",
      "Epoch 133, loss: 736.054586\n",
      "Epoch 134, loss: 732.812314\n",
      "Epoch 135, loss: 834.169416\n",
      "Epoch 136, loss: 797.780813\n",
      "Epoch 137, loss: 818.975310\n",
      "Epoch 138, loss: 685.091874\n",
      "Epoch 139, loss: 661.894364\n",
      "Epoch 140, loss: 793.067400\n",
      "Epoch 141, loss: 772.183995\n",
      "Epoch 142, loss: 743.605972\n",
      "Epoch 143, loss: 795.160279\n",
      "Epoch 144, loss: 706.678059\n",
      "Epoch 145, loss: 808.098281\n",
      "Epoch 146, loss: 642.367584\n",
      "Epoch 147, loss: 731.065099\n",
      "Epoch 148, loss: 647.826888\n",
      "Epoch 149, loss: 674.778056\n",
      "Epoch 150, loss: 617.369062\n",
      "Epoch 151, loss: 931.310294\n",
      "Epoch 152, loss: 787.686846\n",
      "Epoch 153, loss: 936.614565\n",
      "Epoch 154, loss: 689.649080\n",
      "Epoch 155, loss: 734.772538\n",
      "Epoch 156, loss: 785.286599\n",
      "Epoch 157, loss: 801.912746\n",
      "Epoch 158, loss: 685.760021\n",
      "Epoch 159, loss: 835.638042\n",
      "Epoch 160, loss: 769.152968\n",
      "Epoch 161, loss: 705.934845\n",
      "Epoch 162, loss: 944.837938\n",
      "Epoch 163, loss: 623.792246\n",
      "Epoch 164, loss: 727.416095\n",
      "Epoch 165, loss: 611.667935\n",
      "Epoch 166, loss: 697.475590\n",
      "Epoch 167, loss: 950.184640\n",
      "Epoch 168, loss: 784.078791\n",
      "Epoch 169, loss: 884.077722\n",
      "Epoch 170, loss: 771.016210\n",
      "Epoch 171, loss: 719.193830\n",
      "Epoch 172, loss: 734.088113\n",
      "Epoch 173, loss: 708.591352\n",
      "Epoch 174, loss: 747.026739\n",
      "Epoch 175, loss: 747.614645\n",
      "Epoch 176, loss: 703.761641\n",
      "Epoch 177, loss: 755.053809\n",
      "Epoch 178, loss: 779.135268\n",
      "Epoch 179, loss: 693.803157\n",
      "Epoch 180, loss: 738.207988\n",
      "Epoch 181, loss: 718.614487\n",
      "Epoch 182, loss: 652.169695\n",
      "Epoch 183, loss: 741.382837\n",
      "Epoch 184, loss: 690.221755\n",
      "Epoch 185, loss: 638.384707\n",
      "Epoch 186, loss: 862.337104\n",
      "Epoch 187, loss: 690.011443\n",
      "Epoch 188, loss: 647.872095\n",
      "Epoch 189, loss: 739.671160\n",
      "Epoch 190, loss: 694.296133\n",
      "Epoch 191, loss: 677.906817\n",
      "Epoch 192, loss: 654.029378\n",
      "Epoch 193, loss: 738.875392\n",
      "Epoch 194, loss: 868.527102\n",
      "Epoch 195, loss: 779.092689\n",
      "Epoch 196, loss: 619.959221\n",
      "Epoch 197, loss: 905.579633\n",
      "Epoch 198, loss: 603.202194\n",
      "Epoch 199, loss: 684.074470\n",
      "lr: 0.001 | reg: 1e-05 | accuracy: 0.172\n",
      "Epoch 0, loss: 752.506270\n",
      "Epoch 1, loss: 941.622812\n",
      "Epoch 2, loss: 728.525976\n",
      "Epoch 3, loss: 837.272928\n",
      "Epoch 4, loss: 948.122685\n",
      "Epoch 5, loss: 767.425248\n",
      "Epoch 6, loss: 1003.059829\n",
      "Epoch 7, loss: 736.037325\n",
      "Epoch 8, loss: 1006.140025\n",
      "Epoch 9, loss: 907.837904\n",
      "Epoch 10, loss: 742.549574\n",
      "Epoch 11, loss: 993.312248\n",
      "Epoch 12, loss: 854.194080\n",
      "Epoch 13, loss: 867.061924\n",
      "Epoch 14, loss: 844.719515\n",
      "Epoch 15, loss: 1053.931839\n",
      "Epoch 16, loss: 859.833320\n",
      "Epoch 17, loss: 756.296702\n",
      "Epoch 18, loss: 782.727084\n",
      "Epoch 19, loss: 726.944099\n",
      "Epoch 20, loss: 929.671614\n",
      "Epoch 21, loss: 787.610234\n",
      "Epoch 22, loss: 724.245030\n",
      "Epoch 23, loss: 1143.332158\n",
      "Epoch 24, loss: 861.933897\n",
      "Epoch 25, loss: 887.852565\n",
      "Epoch 26, loss: 772.302521\n",
      "Epoch 27, loss: 742.764265\n",
      "Epoch 28, loss: 718.524217\n",
      "Epoch 29, loss: 838.219496\n",
      "Epoch 30, loss: 722.066780\n",
      "Epoch 31, loss: 710.215486\n",
      "Epoch 32, loss: 760.155900\n",
      "Epoch 33, loss: 744.866880\n",
      "Epoch 34, loss: 724.217448\n",
      "Epoch 35, loss: 789.358125\n",
      "Epoch 36, loss: 780.204936\n",
      "Epoch 37, loss: 767.286619\n",
      "Epoch 38, loss: 729.557439\n",
      "Epoch 39, loss: 787.809033\n",
      "Epoch 40, loss: 718.056885\n",
      "Epoch 41, loss: 782.123634\n",
      "Epoch 42, loss: 858.156245\n",
      "Epoch 43, loss: 752.142083\n",
      "Epoch 44, loss: 736.891816\n",
      "Epoch 45, loss: 750.499990\n",
      "Epoch 46, loss: 665.781032\n",
      "Epoch 47, loss: 714.140945\n",
      "Epoch 48, loss: 770.780085\n",
      "Epoch 49, loss: 973.825364\n",
      "Epoch 50, loss: 768.163836\n",
      "Epoch 51, loss: 898.613016\n",
      "Epoch 52, loss: 736.274265\n",
      "Epoch 53, loss: 855.338265\n",
      "Epoch 54, loss: 777.634010\n",
      "Epoch 55, loss: 633.869615\n",
      "Epoch 56, loss: 865.467640\n",
      "Epoch 57, loss: 723.769271\n",
      "Epoch 58, loss: 822.523029\n",
      "Epoch 59, loss: 712.794036\n",
      "Epoch 60, loss: 895.425978\n",
      "Epoch 61, loss: 723.792271\n",
      "Epoch 62, loss: 845.952119\n",
      "Epoch 63, loss: 701.145725\n",
      "Epoch 64, loss: 861.906352\n",
      "Epoch 65, loss: 676.630800\n",
      "Epoch 66, loss: 687.099366\n",
      "Epoch 67, loss: 839.948592\n",
      "Epoch 68, loss: 765.000126\n",
      "Epoch 69, loss: 824.950181\n",
      "Epoch 70, loss: 680.753199\n",
      "Epoch 71, loss: 673.180724\n",
      "Epoch 72, loss: 832.060904\n",
      "Epoch 73, loss: 683.471256\n",
      "Epoch 74, loss: 688.955561\n",
      "Epoch 75, loss: 668.984417\n",
      "Epoch 76, loss: 855.247291\n",
      "Epoch 77, loss: 795.199790\n",
      "Epoch 78, loss: 722.088141\n",
      "Epoch 79, loss: 641.662765\n",
      "Epoch 80, loss: 793.839985\n",
      "Epoch 81, loss: 822.781787\n",
      "Epoch 82, loss: 857.152860\n",
      "Epoch 83, loss: 772.249185\n",
      "Epoch 84, loss: 655.920390\n",
      "Epoch 85, loss: 770.275491\n",
      "Epoch 86, loss: 906.693981\n",
      "Epoch 87, loss: 671.365334\n",
      "Epoch 88, loss: 834.354131\n",
      "Epoch 89, loss: 704.526775\n",
      "Epoch 90, loss: 798.409420\n",
      "Epoch 91, loss: 813.588173\n",
      "Epoch 92, loss: 882.710954\n",
      "Epoch 93, loss: 922.190928\n",
      "Epoch 94, loss: 791.365307\n",
      "Epoch 95, loss: 822.549221\n",
      "Epoch 96, loss: 924.520460\n",
      "Epoch 97, loss: 720.427750\n",
      "Epoch 98, loss: 693.258176\n",
      "Epoch 99, loss: 694.877326\n",
      "Epoch 100, loss: 823.971438\n",
      "Epoch 101, loss: 741.504973\n",
      "Epoch 102, loss: 773.056043\n",
      "Epoch 103, loss: 675.959221\n",
      "Epoch 104, loss: 778.430244\n",
      "Epoch 105, loss: 730.154055\n",
      "Epoch 106, loss: 790.840922\n",
      "Epoch 107, loss: 986.332235\n",
      "Epoch 108, loss: 819.373177\n",
      "Epoch 109, loss: 807.603177\n",
      "Epoch 110, loss: 810.842540\n",
      "Epoch 111, loss: 650.663617\n",
      "Epoch 112, loss: 801.099226\n",
      "Epoch 113, loss: 779.366003\n",
      "Epoch 114, loss: 803.094148\n",
      "Epoch 115, loss: 725.104494\n",
      "Epoch 116, loss: 682.962856\n",
      "Epoch 117, loss: 813.572991\n",
      "Epoch 118, loss: 729.519653\n",
      "Epoch 119, loss: 762.379740\n",
      "Epoch 120, loss: 737.214927\n",
      "Epoch 121, loss: 652.140508\n",
      "Epoch 122, loss: 913.289227\n",
      "Epoch 123, loss: 870.120390\n",
      "Epoch 124, loss: 722.069958\n",
      "Epoch 125, loss: 744.672173\n",
      "Epoch 126, loss: 651.053149\n",
      "Epoch 127, loss: 684.628167\n",
      "Epoch 128, loss: 780.614222\n",
      "Epoch 129, loss: 881.313124\n",
      "Epoch 130, loss: 881.826848\n",
      "Epoch 131, loss: 658.072612\n",
      "Epoch 132, loss: 674.195128\n",
      "Epoch 133, loss: 714.547125\n",
      "Epoch 134, loss: 687.674740\n",
      "Epoch 135, loss: 844.420060\n",
      "Epoch 136, loss: 733.240060\n",
      "Epoch 137, loss: 724.758717\n",
      "Epoch 138, loss: 892.454228\n",
      "Epoch 139, loss: 785.228594\n",
      "Epoch 140, loss: 790.299868\n",
      "Epoch 141, loss: 643.358604\n",
      "Epoch 142, loss: 781.516396\n",
      "Epoch 143, loss: 681.112533\n",
      "Epoch 144, loss: 661.829315\n",
      "Epoch 145, loss: 784.639848\n",
      "Epoch 146, loss: 802.704497\n",
      "Epoch 147, loss: 785.662851\n",
      "Epoch 148, loss: 756.405777\n",
      "Epoch 149, loss: 661.125145\n",
      "Epoch 150, loss: 655.781956\n",
      "Epoch 151, loss: 671.424356\n",
      "Epoch 152, loss: 685.513016\n",
      "Epoch 153, loss: 742.054752\n",
      "Epoch 154, loss: 729.406486\n",
      "Epoch 155, loss: 973.085591\n",
      "Epoch 156, loss: 828.454848\n",
      "Epoch 157, loss: 746.303720\n",
      "Epoch 158, loss: 724.030350\n",
      "Epoch 159, loss: 874.912529\n",
      "Epoch 160, loss: 668.137095\n",
      "Epoch 161, loss: 771.958638\n",
      "Epoch 162, loss: 687.296400\n",
      "Epoch 163, loss: 712.849232\n",
      "Epoch 164, loss: 685.186595\n",
      "Epoch 165, loss: 820.480285\n",
      "Epoch 166, loss: 775.185556\n",
      "Epoch 167, loss: 704.115705\n",
      "Epoch 168, loss: 789.188473\n",
      "Epoch 169, loss: 700.156078\n",
      "Epoch 170, loss: 686.355552\n",
      "Epoch 171, loss: 773.438023\n",
      "Epoch 172, loss: 883.118167\n",
      "Epoch 173, loss: 741.895884\n",
      "Epoch 174, loss: 776.547570\n",
      "Epoch 175, loss: 858.908035\n",
      "Epoch 176, loss: 683.547426\n",
      "Epoch 177, loss: 704.031107\n",
      "Epoch 178, loss: 848.631622\n",
      "Epoch 179, loss: 816.711053\n",
      "Epoch 180, loss: 747.496272\n",
      "Epoch 181, loss: 653.456852\n",
      "Epoch 182, loss: 838.293551\n",
      "Epoch 183, loss: 779.342215\n",
      "Epoch 184, loss: 698.359308\n",
      "Epoch 185, loss: 732.215937\n",
      "Epoch 186, loss: 644.302952\n",
      "Epoch 187, loss: 655.403447\n",
      "Epoch 188, loss: 970.596065\n",
      "Epoch 189, loss: 705.288890\n",
      "Epoch 190, loss: 707.515434\n",
      "Epoch 191, loss: 777.851295\n",
      "Epoch 192, loss: 732.137627\n",
      "Epoch 193, loss: 690.334772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 914.629648\n",
      "Epoch 195, loss: 790.074973\n",
      "Epoch 196, loss: 751.234577\n",
      "Epoch 197, loss: 709.338058\n",
      "Epoch 198, loss: 612.354862\n",
      "Epoch 199, loss: 836.410115\n",
      "lr: 0.001 | reg: 1e-06 | accuracy: 0.19\n",
      "Epoch 0, loss: 685.163770\n",
      "Epoch 1, loss: 683.986255\n",
      "Epoch 2, loss: 675.657039\n",
      "Epoch 3, loss: 669.777845\n",
      "Epoch 4, loss: 660.724673\n",
      "Epoch 5, loss: 657.218116\n",
      "Epoch 6, loss: 656.732199\n",
      "Epoch 7, loss: 660.242819\n",
      "Epoch 8, loss: 664.579931\n",
      "Epoch 9, loss: 652.195792\n",
      "Epoch 10, loss: 659.008568\n",
      "Epoch 11, loss: 658.151015\n",
      "Epoch 12, loss: 648.183107\n",
      "Epoch 13, loss: 656.414724\n",
      "Epoch 14, loss: 649.118946\n",
      "Epoch 15, loss: 648.834570\n",
      "Epoch 16, loss: 654.877723\n",
      "Epoch 17, loss: 636.631721\n",
      "Epoch 18, loss: 645.085025\n",
      "Epoch 19, loss: 654.405399\n",
      "Epoch 20, loss: 650.516884\n",
      "Epoch 21, loss: 646.818052\n",
      "Epoch 22, loss: 641.848432\n",
      "Epoch 23, loss: 649.434512\n",
      "Epoch 24, loss: 648.958114\n",
      "Epoch 25, loss: 637.331693\n",
      "Epoch 26, loss: 644.655102\n",
      "Epoch 27, loss: 647.198315\n",
      "Epoch 28, loss: 633.782746\n",
      "Epoch 29, loss: 651.229518\n",
      "Epoch 30, loss: 645.231436\n",
      "Epoch 31, loss: 651.421030\n",
      "Epoch 32, loss: 645.108183\n",
      "Epoch 33, loss: 650.582550\n",
      "Epoch 34, loss: 634.639907\n",
      "Epoch 35, loss: 641.607939\n",
      "Epoch 36, loss: 631.330379\n",
      "Epoch 37, loss: 645.049834\n",
      "Epoch 38, loss: 637.928362\n",
      "Epoch 39, loss: 619.072648\n",
      "Epoch 40, loss: 641.211141\n",
      "Epoch 41, loss: 652.515411\n",
      "Epoch 42, loss: 635.538399\n",
      "Epoch 43, loss: 631.288835\n",
      "Epoch 44, loss: 633.838309\n",
      "Epoch 45, loss: 646.130667\n",
      "Epoch 46, loss: 641.718155\n",
      "Epoch 47, loss: 623.778245\n",
      "Epoch 48, loss: 621.160342\n",
      "Epoch 49, loss: 631.911978\n",
      "Epoch 50, loss: 623.363212\n",
      "Epoch 51, loss: 631.042037\n",
      "Epoch 52, loss: 630.346345\n",
      "Epoch 53, loss: 648.050376\n",
      "Epoch 54, loss: 638.933611\n",
      "Epoch 55, loss: 641.388701\n",
      "Epoch 56, loss: 644.649342\n",
      "Epoch 57, loss: 624.872548\n",
      "Epoch 58, loss: 620.597741\n",
      "Epoch 59, loss: 631.356166\n",
      "Epoch 60, loss: 629.406985\n",
      "Epoch 61, loss: 626.596513\n",
      "Epoch 62, loss: 622.931113\n",
      "Epoch 63, loss: 617.881283\n",
      "Epoch 64, loss: 637.317051\n",
      "Epoch 65, loss: 642.358939\n",
      "Epoch 66, loss: 627.198061\n",
      "Epoch 67, loss: 623.258180\n",
      "Epoch 68, loss: 651.338778\n",
      "Epoch 69, loss: 628.878096\n",
      "Epoch 70, loss: 628.677915\n",
      "Epoch 71, loss: 626.030258\n",
      "Epoch 72, loss: 633.631529\n",
      "Epoch 73, loss: 619.471146\n",
      "Epoch 74, loss: 630.268989\n",
      "Epoch 75, loss: 645.711664\n",
      "Epoch 76, loss: 616.233698\n",
      "Epoch 77, loss: 635.156942\n",
      "Epoch 78, loss: 625.908903\n",
      "Epoch 79, loss: 622.927505\n",
      "Epoch 80, loss: 627.296734\n",
      "Epoch 81, loss: 616.828327\n",
      "Epoch 82, loss: 629.511450\n",
      "Epoch 83, loss: 625.245184\n",
      "Epoch 84, loss: 633.694383\n",
      "Epoch 85, loss: 628.235325\n",
      "Epoch 86, loss: 620.056120\n",
      "Epoch 87, loss: 623.663482\n",
      "Epoch 88, loss: 617.944372\n",
      "Epoch 89, loss: 620.619148\n",
      "Epoch 90, loss: 624.610846\n",
      "Epoch 91, loss: 634.754741\n",
      "Epoch 92, loss: 653.699655\n",
      "Epoch 93, loss: 608.136469\n",
      "Epoch 94, loss: 626.281342\n",
      "Epoch 95, loss: 638.806227\n",
      "Epoch 96, loss: 627.525673\n",
      "Epoch 97, loss: 623.470589\n",
      "Epoch 98, loss: 630.398654\n",
      "Epoch 99, loss: 625.565880\n",
      "Epoch 100, loss: 637.897653\n",
      "Epoch 101, loss: 621.555514\n",
      "Epoch 102, loss: 639.651886\n",
      "Epoch 103, loss: 634.026847\n",
      "Epoch 104, loss: 630.726076\n",
      "Epoch 105, loss: 628.026890\n",
      "Epoch 106, loss: 614.090736\n",
      "Epoch 107, loss: 618.168508\n",
      "Epoch 108, loss: 624.809951\n",
      "Epoch 109, loss: 620.538923\n",
      "Epoch 110, loss: 597.914183\n",
      "Epoch 111, loss: 638.502911\n",
      "Epoch 112, loss: 617.501914\n",
      "Epoch 113, loss: 621.485090\n",
      "Epoch 114, loss: 630.053520\n",
      "Epoch 115, loss: 635.480712\n",
      "Epoch 116, loss: 629.401105\n",
      "Epoch 117, loss: 620.345642\n",
      "Epoch 118, loss: 623.763541\n",
      "Epoch 119, loss: 612.654866\n",
      "Epoch 120, loss: 616.784725\n",
      "Epoch 121, loss: 627.009780\n",
      "Epoch 122, loss: 628.443298\n",
      "Epoch 123, loss: 637.651028\n",
      "Epoch 124, loss: 637.230944\n",
      "Epoch 125, loss: 626.492399\n",
      "Epoch 126, loss: 614.605206\n",
      "Epoch 127, loss: 637.225512\n",
      "Epoch 128, loss: 635.413218\n",
      "Epoch 129, loss: 613.520608\n",
      "Epoch 130, loss: 625.901269\n",
      "Epoch 131, loss: 599.779056\n",
      "Epoch 132, loss: 618.361334\n",
      "Epoch 133, loss: 623.491922\n",
      "Epoch 134, loss: 629.978619\n",
      "Epoch 135, loss: 633.242624\n",
      "Epoch 136, loss: 625.541809\n",
      "Epoch 137, loss: 612.002011\n",
      "Epoch 138, loss: 617.028390\n",
      "Epoch 139, loss: 625.678817\n",
      "Epoch 140, loss: 628.206087\n",
      "Epoch 141, loss: 619.151319\n",
      "Epoch 142, loss: 594.912697\n",
      "Epoch 143, loss: 608.237016\n",
      "Epoch 144, loss: 627.377536\n",
      "Epoch 145, loss: 638.044793\n",
      "Epoch 146, loss: 633.513415\n",
      "Epoch 147, loss: 614.434233\n",
      "Epoch 148, loss: 608.085174\n",
      "Epoch 149, loss: 611.067038\n",
      "Epoch 150, loss: 615.462484\n",
      "Epoch 151, loss: 617.908431\n",
      "Epoch 152, loss: 604.158224\n",
      "Epoch 153, loss: 615.260998\n",
      "Epoch 154, loss: 617.008281\n",
      "Epoch 155, loss: 622.659971\n",
      "Epoch 156, loss: 624.257442\n",
      "Epoch 157, loss: 617.317562\n",
      "Epoch 158, loss: 619.534292\n",
      "Epoch 159, loss: 638.339471\n",
      "Epoch 160, loss: 629.647439\n",
      "Epoch 161, loss: 627.669530\n",
      "Epoch 162, loss: 612.679653\n",
      "Epoch 163, loss: 626.321583\n",
      "Epoch 164, loss: 633.613585\n",
      "Epoch 165, loss: 614.201564\n",
      "Epoch 166, loss: 642.008990\n",
      "Epoch 167, loss: 632.864082\n",
      "Epoch 168, loss: 627.623748\n",
      "Epoch 169, loss: 643.389964\n",
      "Epoch 170, loss: 630.873582\n",
      "Epoch 171, loss: 618.847827\n",
      "Epoch 172, loss: 610.233464\n",
      "Epoch 173, loss: 630.848957\n",
      "Epoch 174, loss: 623.924917\n",
      "Epoch 175, loss: 608.231905\n",
      "Epoch 176, loss: 625.535843\n",
      "Epoch 177, loss: 616.443462\n",
      "Epoch 178, loss: 605.397814\n",
      "Epoch 179, loss: 605.419986\n",
      "Epoch 180, loss: 604.899955\n",
      "Epoch 181, loss: 605.951597\n",
      "Epoch 182, loss: 605.510641\n",
      "Epoch 183, loss: 632.222825\n",
      "Epoch 184, loss: 644.516911\n",
      "Epoch 185, loss: 622.106650\n",
      "Epoch 186, loss: 618.593733\n",
      "Epoch 187, loss: 627.702598\n",
      "Epoch 188, loss: 622.824907\n",
      "Epoch 189, loss: 636.245052\n",
      "Epoch 190, loss: 613.620559\n",
      "Epoch 191, loss: 628.437779\n",
      "Epoch 192, loss: 609.365236\n",
      "Epoch 193, loss: 609.538863\n",
      "Epoch 194, loss: 621.390451\n",
      "Epoch 195, loss: 622.901369\n",
      "Epoch 196, loss: 613.120667\n",
      "Epoch 197, loss: 615.575762\n",
      "Epoch 198, loss: 624.851684\n",
      "Epoch 199, loss: 624.636242\n",
      "lr: 0.0001 | reg: 0.0001 | accuracy: 0.21\n",
      "Epoch 0, loss: 686.253825\n",
      "Epoch 1, loss: 680.222169\n",
      "Epoch 2, loss: 675.740703\n",
      "Epoch 3, loss: 673.459532\n",
      "Epoch 4, loss: 669.648599\n",
      "Epoch 5, loss: 663.484138\n",
      "Epoch 6, loss: 671.466991\n",
      "Epoch 7, loss: 664.394112\n",
      "Epoch 8, loss: 653.554204\n",
      "Epoch 9, loss: 655.479143\n",
      "Epoch 10, loss: 647.893978\n",
      "Epoch 11, loss: 647.759374\n",
      "Epoch 12, loss: 650.846016\n",
      "Epoch 13, loss: 643.718339\n",
      "Epoch 14, loss: 642.208482\n",
      "Epoch 15, loss: 654.346570\n",
      "Epoch 16, loss: 647.964453\n",
      "Epoch 17, loss: 636.542786\n",
      "Epoch 18, loss: 641.393661\n",
      "Epoch 19, loss: 644.301834\n",
      "Epoch 20, loss: 643.074825\n",
      "Epoch 21, loss: 647.692257\n",
      "Epoch 22, loss: 656.884098\n",
      "Epoch 23, loss: 643.095901\n",
      "Epoch 24, loss: 648.676521\n",
      "Epoch 25, loss: 638.879576\n",
      "Epoch 26, loss: 632.742236\n",
      "Epoch 27, loss: 645.716475\n",
      "Epoch 28, loss: 644.967188\n",
      "Epoch 29, loss: 639.364239\n",
      "Epoch 30, loss: 622.740281\n",
      "Epoch 31, loss: 649.600739\n",
      "Epoch 32, loss: 645.839922\n",
      "Epoch 33, loss: 632.332378\n",
      "Epoch 34, loss: 639.327630\n",
      "Epoch 35, loss: 656.298058\n",
      "Epoch 36, loss: 655.102786\n",
      "Epoch 37, loss: 632.079051\n",
      "Epoch 38, loss: 646.088592\n",
      "Epoch 39, loss: 634.361834\n",
      "Epoch 40, loss: 644.229298\n",
      "Epoch 41, loss: 630.786179\n",
      "Epoch 42, loss: 642.019216\n",
      "Epoch 43, loss: 628.542733\n",
      "Epoch 44, loss: 639.314736\n",
      "Epoch 45, loss: 643.461854\n",
      "Epoch 46, loss: 634.985743\n",
      "Epoch 47, loss: 635.011351\n",
      "Epoch 48, loss: 641.935453\n",
      "Epoch 49, loss: 637.327167\n",
      "Epoch 50, loss: 631.929455\n",
      "Epoch 51, loss: 640.251254\n",
      "Epoch 52, loss: 636.915028\n",
      "Epoch 53, loss: 634.286856\n",
      "Epoch 54, loss: 631.975485\n",
      "Epoch 55, loss: 641.725464\n",
      "Epoch 56, loss: 634.709922\n",
      "Epoch 57, loss: 617.072763\n",
      "Epoch 58, loss: 637.837958\n",
      "Epoch 59, loss: 636.111916\n",
      "Epoch 60, loss: 644.499624\n",
      "Epoch 61, loss: 638.088173\n",
      "Epoch 62, loss: 635.443790\n",
      "Epoch 63, loss: 620.299438\n",
      "Epoch 64, loss: 618.239641\n",
      "Epoch 65, loss: 612.793461\n",
      "Epoch 66, loss: 640.475654\n",
      "Epoch 67, loss: 625.973735\n",
      "Epoch 68, loss: 640.434578\n",
      "Epoch 69, loss: 622.985722\n",
      "Epoch 70, loss: 639.728308\n",
      "Epoch 71, loss: 622.101181\n",
      "Epoch 72, loss: 638.027659\n",
      "Epoch 73, loss: 641.222541\n",
      "Epoch 74, loss: 630.278201\n",
      "Epoch 75, loss: 637.698345\n",
      "Epoch 76, loss: 635.890576\n",
      "Epoch 77, loss: 631.025062\n",
      "Epoch 78, loss: 625.231248\n",
      "Epoch 79, loss: 620.386087\n",
      "Epoch 80, loss: 628.789218\n",
      "Epoch 81, loss: 616.140849\n",
      "Epoch 82, loss: 608.337345\n",
      "Epoch 83, loss: 628.131489\n",
      "Epoch 84, loss: 633.189715\n",
      "Epoch 85, loss: 628.286582\n",
      "Epoch 86, loss: 619.582488\n",
      "Epoch 87, loss: 638.253865\n",
      "Epoch 88, loss: 639.120850\n",
      "Epoch 89, loss: 628.032029\n",
      "Epoch 90, loss: 631.483140\n",
      "Epoch 91, loss: 624.908825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 638.529020\n",
      "Epoch 93, loss: 619.949271\n",
      "Epoch 94, loss: 631.578615\n",
      "Epoch 95, loss: 639.732847\n",
      "Epoch 96, loss: 598.230506\n",
      "Epoch 97, loss: 628.513638\n",
      "Epoch 98, loss: 619.641578\n",
      "Epoch 99, loss: 620.557398\n",
      "Epoch 100, loss: 645.117289\n",
      "Epoch 101, loss: 594.870588\n",
      "Epoch 102, loss: 622.832114\n",
      "Epoch 103, loss: 638.082057\n",
      "Epoch 104, loss: 616.658229\n",
      "Epoch 105, loss: 655.150379\n",
      "Epoch 106, loss: 621.668130\n",
      "Epoch 107, loss: 619.134337\n",
      "Epoch 108, loss: 630.653482\n",
      "Epoch 109, loss: 611.328678\n",
      "Epoch 110, loss: 619.170390\n",
      "Epoch 111, loss: 629.842751\n",
      "Epoch 112, loss: 618.030007\n",
      "Epoch 113, loss: 631.498036\n",
      "Epoch 114, loss: 603.855743\n",
      "Epoch 115, loss: 618.363317\n",
      "Epoch 116, loss: 621.060856\n",
      "Epoch 117, loss: 626.605695\n",
      "Epoch 118, loss: 635.780643\n",
      "Epoch 119, loss: 618.470750\n",
      "Epoch 120, loss: 626.005827\n",
      "Epoch 121, loss: 623.372152\n",
      "Epoch 122, loss: 608.728047\n",
      "Epoch 123, loss: 624.127515\n",
      "Epoch 124, loss: 638.607081\n",
      "Epoch 125, loss: 631.606696\n",
      "Epoch 126, loss: 605.390918\n",
      "Epoch 127, loss: 626.504229\n",
      "Epoch 128, loss: 604.456934\n",
      "Epoch 129, loss: 629.365251\n",
      "Epoch 130, loss: 635.267104\n",
      "Epoch 131, loss: 625.728752\n",
      "Epoch 132, loss: 622.343444\n",
      "Epoch 133, loss: 629.429543\n",
      "Epoch 134, loss: 611.428236\n",
      "Epoch 135, loss: 626.613172\n",
      "Epoch 136, loss: 639.454085\n",
      "Epoch 137, loss: 599.817562\n",
      "Epoch 138, loss: 620.846593\n",
      "Epoch 139, loss: 618.962092\n",
      "Epoch 140, loss: 624.069793\n",
      "Epoch 141, loss: 623.736384\n",
      "Epoch 142, loss: 625.000317\n",
      "Epoch 143, loss: 622.149087\n",
      "Epoch 144, loss: 647.925309\n",
      "Epoch 145, loss: 628.023936\n",
      "Epoch 146, loss: 640.618252\n",
      "Epoch 147, loss: 611.886765\n",
      "Epoch 148, loss: 614.481506\n",
      "Epoch 149, loss: 622.839832\n",
      "Epoch 150, loss: 615.065873\n",
      "Epoch 151, loss: 623.923665\n",
      "Epoch 152, loss: 623.399240\n",
      "Epoch 153, loss: 626.622940\n",
      "Epoch 154, loss: 631.209068\n",
      "Epoch 155, loss: 619.189173\n",
      "Epoch 156, loss: 620.173786\n",
      "Epoch 157, loss: 629.508891\n",
      "Epoch 158, loss: 591.008090\n",
      "Epoch 159, loss: 625.515331\n",
      "Epoch 160, loss: 616.870529\n",
      "Epoch 161, loss: 622.832535\n",
      "Epoch 162, loss: 613.746836\n",
      "Epoch 163, loss: 626.342392\n",
      "Epoch 164, loss: 617.675122\n",
      "Epoch 165, loss: 613.400489\n",
      "Epoch 166, loss: 618.140280\n",
      "Epoch 167, loss: 618.453076\n",
      "Epoch 168, loss: 634.169807\n",
      "Epoch 169, loss: 616.548303\n",
      "Epoch 170, loss: 610.923561\n",
      "Epoch 171, loss: 604.075925\n",
      "Epoch 172, loss: 633.266648\n",
      "Epoch 173, loss: 602.729691\n",
      "Epoch 174, loss: 625.763921\n",
      "Epoch 175, loss: 621.820866\n",
      "Epoch 176, loss: 610.213952\n",
      "Epoch 177, loss: 617.393754\n",
      "Epoch 178, loss: 620.681687\n",
      "Epoch 179, loss: 605.927218\n",
      "Epoch 180, loss: 620.084876\n",
      "Epoch 181, loss: 623.818878\n",
      "Epoch 182, loss: 612.456995\n",
      "Epoch 183, loss: 593.295690\n",
      "Epoch 184, loss: 615.481969\n",
      "Epoch 185, loss: 619.957536\n",
      "Epoch 186, loss: 619.379803\n",
      "Epoch 187, loss: 624.541650\n",
      "Epoch 188, loss: 617.952274\n",
      "Epoch 189, loss: 609.965387\n",
      "Epoch 190, loss: 618.783008\n",
      "Epoch 191, loss: 618.704962\n",
      "Epoch 192, loss: 603.271749\n",
      "Epoch 193, loss: 608.212000\n",
      "Epoch 194, loss: 608.299305\n",
      "Epoch 195, loss: 635.606266\n",
      "Epoch 196, loss: 657.779176\n",
      "Epoch 197, loss: 619.478952\n",
      "Epoch 198, loss: 607.674294\n",
      "Epoch 199, loss: 619.010362\n",
      "lr: 0.0001 | reg: 1e-05 | accuracy: 0.206\n",
      "Epoch 0, loss: 685.935932\n",
      "Epoch 1, loss: 680.635184\n",
      "Epoch 2, loss: 673.127411\n",
      "Epoch 3, loss: 677.815327\n",
      "Epoch 4, loss: 669.262010\n",
      "Epoch 5, loss: 666.211297\n",
      "Epoch 6, loss: 665.208429\n",
      "Epoch 7, loss: 659.113734\n",
      "Epoch 8, loss: 664.510386\n",
      "Epoch 9, loss: 650.769410\n",
      "Epoch 10, loss: 658.788967\n",
      "Epoch 11, loss: 661.133555\n",
      "Epoch 12, loss: 652.040909\n",
      "Epoch 13, loss: 646.911319\n",
      "Epoch 14, loss: 656.592247\n",
      "Epoch 15, loss: 648.431346\n",
      "Epoch 16, loss: 650.455364\n",
      "Epoch 17, loss: 658.170405\n",
      "Epoch 18, loss: 647.889236\n",
      "Epoch 19, loss: 639.600479\n",
      "Epoch 20, loss: 639.931827\n",
      "Epoch 21, loss: 636.140622\n",
      "Epoch 22, loss: 649.748222\n",
      "Epoch 23, loss: 643.199618\n",
      "Epoch 24, loss: 626.386179\n",
      "Epoch 25, loss: 638.564786\n",
      "Epoch 26, loss: 647.301603\n",
      "Epoch 27, loss: 656.516974\n",
      "Epoch 28, loss: 638.014215\n",
      "Epoch 29, loss: 638.940516\n",
      "Epoch 30, loss: 630.104095\n",
      "Epoch 31, loss: 637.480001\n",
      "Epoch 32, loss: 651.678601\n",
      "Epoch 33, loss: 631.241860\n",
      "Epoch 34, loss: 652.168652\n",
      "Epoch 35, loss: 621.578089\n",
      "Epoch 36, loss: 646.050933\n",
      "Epoch 37, loss: 631.829609\n",
      "Epoch 38, loss: 628.996738\n",
      "Epoch 39, loss: 637.709910\n",
      "Epoch 40, loss: 630.501695\n",
      "Epoch 41, loss: 618.832938\n",
      "Epoch 42, loss: 627.886216\n",
      "Epoch 43, loss: 632.114710\n",
      "Epoch 44, loss: 638.122083\n",
      "Epoch 45, loss: 632.998744\n",
      "Epoch 46, loss: 654.770232\n",
      "Epoch 47, loss: 629.130454\n",
      "Epoch 48, loss: 654.719423\n",
      "Epoch 49, loss: 640.030637\n",
      "Epoch 50, loss: 637.386901\n",
      "Epoch 51, loss: 639.126567\n",
      "Epoch 52, loss: 636.707013\n",
      "Epoch 53, loss: 630.172864\n",
      "Epoch 54, loss: 624.872669\n",
      "Epoch 55, loss: 633.787764\n",
      "Epoch 56, loss: 615.589849\n",
      "Epoch 57, loss: 638.885468\n",
      "Epoch 58, loss: 647.258124\n",
      "Epoch 59, loss: 633.443326\n",
      "Epoch 60, loss: 611.454329\n",
      "Epoch 61, loss: 634.712903\n",
      "Epoch 62, loss: 636.989541\n",
      "Epoch 63, loss: 635.341333\n",
      "Epoch 64, loss: 629.277118\n",
      "Epoch 65, loss: 644.204472\n",
      "Epoch 66, loss: 626.676510\n",
      "Epoch 67, loss: 621.336473\n",
      "Epoch 68, loss: 615.978457\n",
      "Epoch 69, loss: 632.450385\n",
      "Epoch 70, loss: 627.544270\n",
      "Epoch 71, loss: 642.601033\n",
      "Epoch 72, loss: 630.748815\n",
      "Epoch 73, loss: 651.738587\n",
      "Epoch 74, loss: 628.202401\n",
      "Epoch 75, loss: 642.991591\n",
      "Epoch 76, loss: 638.157766\n",
      "Epoch 77, loss: 623.741791\n",
      "Epoch 78, loss: 627.158976\n",
      "Epoch 79, loss: 626.356639\n",
      "Epoch 80, loss: 628.443442\n",
      "Epoch 81, loss: 634.254233\n",
      "Epoch 82, loss: 627.606240\n",
      "Epoch 83, loss: 642.219799\n",
      "Epoch 84, loss: 638.842600\n",
      "Epoch 85, loss: 652.509352\n",
      "Epoch 86, loss: 619.351633\n",
      "Epoch 87, loss: 627.405776\n",
      "Epoch 88, loss: 623.420273\n",
      "Epoch 89, loss: 615.479501\n",
      "Epoch 90, loss: 645.697466\n",
      "Epoch 91, loss: 623.970816\n",
      "Epoch 92, loss: 637.963193\n",
      "Epoch 93, loss: 607.746687\n",
      "Epoch 94, loss: 617.484846\n",
      "Epoch 95, loss: 647.598295\n",
      "Epoch 96, loss: 641.063902\n",
      "Epoch 97, loss: 637.056580\n",
      "Epoch 98, loss: 630.917160\n",
      "Epoch 99, loss: 618.321107\n",
      "Epoch 100, loss: 625.916066\n",
      "Epoch 101, loss: 616.160954\n",
      "Epoch 102, loss: 613.948861\n",
      "Epoch 103, loss: 625.019671\n",
      "Epoch 104, loss: 643.314008\n",
      "Epoch 105, loss: 609.040661\n",
      "Epoch 106, loss: 627.915669\n",
      "Epoch 107, loss: 636.497468\n",
      "Epoch 108, loss: 625.562644\n",
      "Epoch 109, loss: 612.797877\n",
      "Epoch 110, loss: 626.610769\n",
      "Epoch 111, loss: 627.270041\n",
      "Epoch 112, loss: 630.984895\n",
      "Epoch 113, loss: 623.585425\n",
      "Epoch 114, loss: 621.698307\n",
      "Epoch 115, loss: 633.149662\n",
      "Epoch 116, loss: 610.511865\n",
      "Epoch 117, loss: 617.931103\n",
      "Epoch 118, loss: 629.422987\n",
      "Epoch 119, loss: 628.210566\n",
      "Epoch 120, loss: 636.559006\n",
      "Epoch 121, loss: 628.896788\n",
      "Epoch 122, loss: 606.703001\n",
      "Epoch 123, loss: 617.005895\n",
      "Epoch 124, loss: 619.843289\n",
      "Epoch 125, loss: 633.275477\n",
      "Epoch 126, loss: 623.896321\n",
      "Epoch 127, loss: 623.016169\n",
      "Epoch 128, loss: 627.201394\n",
      "Epoch 129, loss: 613.053822\n",
      "Epoch 130, loss: 609.081080\n",
      "Epoch 131, loss: 621.728821\n",
      "Epoch 132, loss: 626.062188\n",
      "Epoch 133, loss: 621.404970\n",
      "Epoch 134, loss: 635.569843\n",
      "Epoch 135, loss: 631.720015\n",
      "Epoch 136, loss: 622.656623\n",
      "Epoch 137, loss: 622.358615\n",
      "Epoch 138, loss: 618.387393\n",
      "Epoch 139, loss: 622.254174\n",
      "Epoch 140, loss: 625.625607\n",
      "Epoch 141, loss: 635.279398\n",
      "Epoch 142, loss: 610.462202\n",
      "Epoch 143, loss: 628.699222\n",
      "Epoch 144, loss: 631.091838\n",
      "Epoch 145, loss: 624.725956\n",
      "Epoch 146, loss: 634.358918\n",
      "Epoch 147, loss: 613.512707\n",
      "Epoch 148, loss: 624.713988\n",
      "Epoch 149, loss: 615.718152\n",
      "Epoch 150, loss: 628.801484\n",
      "Epoch 151, loss: 610.806292\n",
      "Epoch 152, loss: 618.457584\n",
      "Epoch 153, loss: 625.757135\n",
      "Epoch 154, loss: 616.447537\n",
      "Epoch 155, loss: 623.178126\n",
      "Epoch 156, loss: 632.759497\n",
      "Epoch 157, loss: 624.579790\n",
      "Epoch 158, loss: 622.970671\n",
      "Epoch 159, loss: 603.134899\n",
      "Epoch 160, loss: 629.724606\n",
      "Epoch 161, loss: 625.224226\n",
      "Epoch 162, loss: 629.115322\n",
      "Epoch 163, loss: 619.406349\n",
      "Epoch 164, loss: 620.559289\n",
      "Epoch 165, loss: 621.784649\n",
      "Epoch 166, loss: 628.507577\n",
      "Epoch 167, loss: 612.023940\n",
      "Epoch 168, loss: 619.933419\n",
      "Epoch 169, loss: 601.663094\n",
      "Epoch 170, loss: 632.314796\n",
      "Epoch 171, loss: 613.991394\n",
      "Epoch 172, loss: 621.908211\n",
      "Epoch 173, loss: 616.903487\n",
      "Epoch 174, loss: 632.555791\n",
      "Epoch 175, loss: 615.037530\n",
      "Epoch 176, loss: 607.036665\n",
      "Epoch 177, loss: 603.555510\n",
      "Epoch 178, loss: 624.683947\n",
      "Epoch 179, loss: 622.396314\n",
      "Epoch 180, loss: 604.568059\n",
      "Epoch 181, loss: 612.304698\n",
      "Epoch 182, loss: 620.284924\n",
      "Epoch 183, loss: 621.271227\n",
      "Epoch 184, loss: 618.947044\n",
      "Epoch 185, loss: 627.020371\n",
      "Epoch 186, loss: 633.349608\n",
      "Epoch 187, loss: 622.401246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, loss: 624.099201\n",
      "Epoch 189, loss: 608.176395\n",
      "Epoch 190, loss: 641.126490\n",
      "Epoch 191, loss: 606.045716\n",
      "Epoch 192, loss: 629.149751\n",
      "Epoch 193, loss: 604.601960\n",
      "Epoch 194, loss: 608.048714\n",
      "Epoch 195, loss: 615.113343\n",
      "Epoch 196, loss: 620.594157\n",
      "Epoch 197, loss: 603.251279\n",
      "Epoch 198, loss: 631.075113\n",
      "Epoch 199, loss: 608.342517\n",
      "lr: 0.0001 | reg: 1e-06 | accuracy: 0.21\n",
      "Epoch 0, loss: 690.362244\n",
      "Epoch 1, loss: 688.978623\n",
      "Epoch 2, loss: 688.115010\n",
      "Epoch 3, loss: 687.514843\n",
      "Epoch 4, loss: 688.148291\n",
      "Epoch 5, loss: 687.988238\n",
      "Epoch 6, loss: 684.074412\n",
      "Epoch 7, loss: 685.999036\n",
      "Epoch 8, loss: 685.447598\n",
      "Epoch 9, loss: 683.321331\n",
      "Epoch 10, loss: 681.958423\n",
      "Epoch 11, loss: 682.473589\n",
      "Epoch 12, loss: 683.254504\n",
      "Epoch 13, loss: 682.130547\n",
      "Epoch 14, loss: 682.097360\n",
      "Epoch 15, loss: 680.427195\n",
      "Epoch 16, loss: 677.151275\n",
      "Epoch 17, loss: 677.234800\n",
      "Epoch 18, loss: 678.274290\n",
      "Epoch 19, loss: 677.157372\n",
      "Epoch 20, loss: 674.489287\n",
      "Epoch 21, loss: 675.535314\n",
      "Epoch 22, loss: 674.067107\n",
      "Epoch 23, loss: 676.133541\n",
      "Epoch 24, loss: 674.326045\n",
      "Epoch 25, loss: 676.652691\n",
      "Epoch 26, loss: 676.303281\n",
      "Epoch 27, loss: 671.451074\n",
      "Epoch 28, loss: 674.700825\n",
      "Epoch 29, loss: 670.779889\n",
      "Epoch 30, loss: 677.288725\n",
      "Epoch 31, loss: 674.838768\n",
      "Epoch 32, loss: 677.649810\n",
      "Epoch 33, loss: 673.782582\n",
      "Epoch 34, loss: 668.096073\n",
      "Epoch 35, loss: 668.935862\n",
      "Epoch 36, loss: 670.536509\n",
      "Epoch 37, loss: 666.632687\n",
      "Epoch 38, loss: 664.252059\n",
      "Epoch 39, loss: 670.291874\n",
      "Epoch 40, loss: 670.697026\n",
      "Epoch 41, loss: 669.369129\n",
      "Epoch 42, loss: 668.081024\n",
      "Epoch 43, loss: 671.525017\n",
      "Epoch 44, loss: 664.618741\n",
      "Epoch 45, loss: 664.624417\n",
      "Epoch 46, loss: 659.724913\n",
      "Epoch 47, loss: 675.135250\n",
      "Epoch 48, loss: 666.696730\n",
      "Epoch 49, loss: 666.090069\n",
      "Epoch 50, loss: 666.061954\n",
      "Epoch 51, loss: 663.536829\n",
      "Epoch 52, loss: 665.235259\n",
      "Epoch 53, loss: 667.966375\n",
      "Epoch 54, loss: 666.170440\n",
      "Epoch 55, loss: 667.971500\n",
      "Epoch 56, loss: 669.476184\n",
      "Epoch 57, loss: 660.290323\n",
      "Epoch 58, loss: 661.111231\n",
      "Epoch 59, loss: 662.079505\n",
      "Epoch 60, loss: 664.893963\n",
      "Epoch 61, loss: 657.767503\n",
      "Epoch 62, loss: 666.881490\n",
      "Epoch 63, loss: 662.744625\n",
      "Epoch 64, loss: 659.851056\n",
      "Epoch 65, loss: 663.953797\n",
      "Epoch 66, loss: 659.786234\n",
      "Epoch 67, loss: 670.677007\n",
      "Epoch 68, loss: 658.746834\n",
      "Epoch 69, loss: 662.601203\n",
      "Epoch 70, loss: 666.682956\n",
      "Epoch 71, loss: 662.710481\n",
      "Epoch 72, loss: 655.680923\n",
      "Epoch 73, loss: 656.387461\n",
      "Epoch 74, loss: 651.558516\n",
      "Epoch 75, loss: 654.893664\n",
      "Epoch 76, loss: 662.691498\n",
      "Epoch 77, loss: 666.474005\n",
      "Epoch 78, loss: 658.313659\n",
      "Epoch 79, loss: 658.279023\n",
      "Epoch 80, loss: 658.867940\n",
      "Epoch 81, loss: 653.494348\n",
      "Epoch 82, loss: 662.400109\n",
      "Epoch 83, loss: 659.694111\n",
      "Epoch 84, loss: 649.097364\n",
      "Epoch 85, loss: 647.357380\n",
      "Epoch 86, loss: 661.734562\n",
      "Epoch 87, loss: 658.243532\n",
      "Epoch 88, loss: 655.724062\n",
      "Epoch 89, loss: 659.154030\n",
      "Epoch 90, loss: 642.190157\n",
      "Epoch 91, loss: 646.400019\n",
      "Epoch 92, loss: 649.719965\n",
      "Epoch 93, loss: 654.327293\n",
      "Epoch 94, loss: 660.907487\n",
      "Epoch 95, loss: 654.850685\n",
      "Epoch 96, loss: 654.435783\n",
      "Epoch 97, loss: 655.588710\n",
      "Epoch 98, loss: 647.980116\n",
      "Epoch 99, loss: 663.331534\n",
      "Epoch 100, loss: 654.751982\n",
      "Epoch 101, loss: 652.586043\n",
      "Epoch 102, loss: 649.781864\n",
      "Epoch 103, loss: 662.634768\n",
      "Epoch 104, loss: 654.993531\n",
      "Epoch 105, loss: 649.218696\n",
      "Epoch 106, loss: 650.012231\n",
      "Epoch 107, loss: 646.878730\n",
      "Epoch 108, loss: 655.915153\n",
      "Epoch 109, loss: 661.261234\n",
      "Epoch 110, loss: 645.103077\n",
      "Epoch 111, loss: 650.531740\n",
      "Epoch 112, loss: 654.686505\n",
      "Epoch 113, loss: 660.273642\n",
      "Epoch 114, loss: 651.210839\n",
      "Epoch 115, loss: 650.910221\n",
      "Epoch 116, loss: 653.677112\n",
      "Epoch 117, loss: 641.509385\n",
      "Epoch 118, loss: 643.663776\n",
      "Epoch 119, loss: 667.084512\n",
      "Epoch 120, loss: 652.364060\n",
      "Epoch 121, loss: 655.278415\n",
      "Epoch 122, loss: 660.046488\n",
      "Epoch 123, loss: 645.378090\n",
      "Epoch 124, loss: 650.419347\n",
      "Epoch 125, loss: 652.709851\n",
      "Epoch 126, loss: 638.092157\n",
      "Epoch 127, loss: 660.700229\n",
      "Epoch 128, loss: 648.893853\n",
      "Epoch 129, loss: 658.548731\n",
      "Epoch 130, loss: 669.812158\n",
      "Epoch 131, loss: 653.044987\n",
      "Epoch 132, loss: 647.481877\n",
      "Epoch 133, loss: 648.045451\n",
      "Epoch 134, loss: 644.314958\n",
      "Epoch 135, loss: 646.615325\n",
      "Epoch 136, loss: 647.317052\n",
      "Epoch 137, loss: 653.265895\n",
      "Epoch 138, loss: 644.946750\n",
      "Epoch 139, loss: 654.405776\n",
      "Epoch 140, loss: 644.607572\n",
      "Epoch 141, loss: 659.141250\n",
      "Epoch 142, loss: 651.771775\n",
      "Epoch 143, loss: 645.750576\n",
      "Epoch 144, loss: 648.659619\n",
      "Epoch 145, loss: 649.132345\n",
      "Epoch 146, loss: 647.971537\n",
      "Epoch 147, loss: 655.982848\n",
      "Epoch 148, loss: 658.994781\n",
      "Epoch 149, loss: 650.102330\n",
      "Epoch 150, loss: 639.791916\n",
      "Epoch 151, loss: 643.475932\n",
      "Epoch 152, loss: 640.490718\n",
      "Epoch 153, loss: 653.694469\n",
      "Epoch 154, loss: 640.589006\n",
      "Epoch 155, loss: 649.284805\n",
      "Epoch 156, loss: 645.505676\n",
      "Epoch 157, loss: 653.128516\n",
      "Epoch 158, loss: 658.493503\n",
      "Epoch 159, loss: 649.152436\n",
      "Epoch 160, loss: 636.260022\n",
      "Epoch 161, loss: 658.700021\n",
      "Epoch 162, loss: 646.470405\n",
      "Epoch 163, loss: 640.332213\n",
      "Epoch 164, loss: 642.883260\n",
      "Epoch 165, loss: 653.117415\n",
      "Epoch 166, loss: 662.983681\n",
      "Epoch 167, loss: 644.648288\n",
      "Epoch 168, loss: 648.221017\n",
      "Epoch 169, loss: 645.952394\n",
      "Epoch 170, loss: 636.147953\n",
      "Epoch 171, loss: 658.938329\n",
      "Epoch 172, loss: 642.457519\n",
      "Epoch 173, loss: 655.644741\n",
      "Epoch 174, loss: 648.838984\n",
      "Epoch 175, loss: 651.373879\n",
      "Epoch 176, loss: 651.863534\n",
      "Epoch 177, loss: 646.881129\n",
      "Epoch 178, loss: 636.569859\n",
      "Epoch 179, loss: 652.908590\n",
      "Epoch 180, loss: 642.957519\n",
      "Epoch 181, loss: 653.484761\n",
      "Epoch 182, loss: 649.542574\n",
      "Epoch 183, loss: 658.171714\n",
      "Epoch 184, loss: 643.868881\n",
      "Epoch 185, loss: 642.575467\n",
      "Epoch 186, loss: 640.338685\n",
      "Epoch 187, loss: 651.298386\n",
      "Epoch 188, loss: 641.105939\n",
      "Epoch 189, loss: 637.545877\n",
      "Epoch 190, loss: 640.533987\n",
      "Epoch 191, loss: 632.274767\n",
      "Epoch 192, loss: 644.728999\n",
      "Epoch 193, loss: 663.011121\n",
      "Epoch 194, loss: 644.949513\n",
      "Epoch 195, loss: 651.879725\n",
      "Epoch 196, loss: 646.564312\n",
      "Epoch 197, loss: 650.988512\n",
      "Epoch 198, loss: 645.929245\n",
      "Epoch 199, loss: 652.079482\n",
      "lr: 1e-05 | reg: 0.0001 | accuracy: 0.204\n",
      "Epoch 0, loss: 690.234359\n",
      "Epoch 1, loss: 690.102840\n",
      "Epoch 2, loss: 689.099230\n",
      "Epoch 3, loss: 687.250732\n",
      "Epoch 4, loss: 686.581917\n",
      "Epoch 5, loss: 688.940084\n",
      "Epoch 6, loss: 685.712413\n",
      "Epoch 7, loss: 684.770982\n",
      "Epoch 8, loss: 685.549768\n",
      "Epoch 9, loss: 683.229381\n",
      "Epoch 10, loss: 682.140334\n",
      "Epoch 11, loss: 685.133653\n",
      "Epoch 12, loss: 683.039261\n",
      "Epoch 13, loss: 685.536393\n",
      "Epoch 14, loss: 679.774673\n",
      "Epoch 15, loss: 683.088780\n",
      "Epoch 16, loss: 676.959844\n",
      "Epoch 17, loss: 677.082029\n",
      "Epoch 18, loss: 678.306501\n",
      "Epoch 19, loss: 674.851767\n",
      "Epoch 20, loss: 676.814366\n",
      "Epoch 21, loss: 676.769699\n",
      "Epoch 22, loss: 675.786803\n",
      "Epoch 23, loss: 678.640215\n",
      "Epoch 24, loss: 672.771461\n",
      "Epoch 25, loss: 672.833340\n",
      "Epoch 26, loss: 671.584455\n",
      "Epoch 27, loss: 677.417634\n",
      "Epoch 28, loss: 673.946592\n",
      "Epoch 29, loss: 671.804113\n",
      "Epoch 30, loss: 669.725263\n",
      "Epoch 31, loss: 676.937145\n",
      "Epoch 32, loss: 675.736901\n",
      "Epoch 33, loss: 670.911552\n",
      "Epoch 34, loss: 673.595017\n",
      "Epoch 35, loss: 673.654170\n",
      "Epoch 36, loss: 675.793052\n",
      "Epoch 37, loss: 676.554889\n",
      "Epoch 38, loss: 667.394271\n",
      "Epoch 39, loss: 673.650420\n",
      "Epoch 40, loss: 664.963801\n",
      "Epoch 41, loss: 665.812229\n",
      "Epoch 42, loss: 668.377499\n",
      "Epoch 43, loss: 666.005427\n",
      "Epoch 44, loss: 663.693769\n",
      "Epoch 45, loss: 666.051640\n",
      "Epoch 46, loss: 660.751265\n",
      "Epoch 47, loss: 664.543830\n",
      "Epoch 48, loss: 664.494294\n",
      "Epoch 49, loss: 672.169630\n",
      "Epoch 50, loss: 660.798174\n",
      "Epoch 51, loss: 667.764718\n",
      "Epoch 52, loss: 663.461203\n",
      "Epoch 53, loss: 661.037734\n",
      "Epoch 54, loss: 668.884334\n",
      "Epoch 55, loss: 662.771155\n",
      "Epoch 56, loss: 660.922431\n",
      "Epoch 57, loss: 661.158519\n",
      "Epoch 58, loss: 657.088786\n",
      "Epoch 59, loss: 663.126700\n",
      "Epoch 60, loss: 658.767301\n",
      "Epoch 61, loss: 659.805449\n",
      "Epoch 62, loss: 658.657455\n",
      "Epoch 63, loss: 652.306808\n",
      "Epoch 64, loss: 662.187271\n",
      "Epoch 65, loss: 664.044741\n",
      "Epoch 66, loss: 658.212789\n",
      "Epoch 67, loss: 660.821116\n",
      "Epoch 68, loss: 664.403067\n",
      "Epoch 69, loss: 666.982496\n",
      "Epoch 70, loss: 661.415845\n",
      "Epoch 71, loss: 662.374419\n",
      "Epoch 72, loss: 655.385134\n",
      "Epoch 73, loss: 648.872337\n",
      "Epoch 74, loss: 650.716925\n",
      "Epoch 75, loss: 652.876801\n",
      "Epoch 76, loss: 662.003856\n",
      "Epoch 77, loss: 653.091223\n",
      "Epoch 78, loss: 647.247271\n",
      "Epoch 79, loss: 658.084251\n",
      "Epoch 80, loss: 658.436755\n",
      "Epoch 81, loss: 664.621819\n",
      "Epoch 82, loss: 658.989286\n",
      "Epoch 83, loss: 647.942015\n",
      "Epoch 84, loss: 652.734072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, loss: 659.867759\n",
      "Epoch 86, loss: 666.035581\n",
      "Epoch 87, loss: 658.658720\n",
      "Epoch 88, loss: 656.716610\n",
      "Epoch 89, loss: 652.744353\n",
      "Epoch 90, loss: 652.504836\n",
      "Epoch 91, loss: 647.519122\n",
      "Epoch 92, loss: 654.345663\n",
      "Epoch 93, loss: 662.599108\n",
      "Epoch 94, loss: 666.340670\n",
      "Epoch 95, loss: 657.663261\n",
      "Epoch 96, loss: 656.186320\n",
      "Epoch 97, loss: 651.255970\n",
      "Epoch 98, loss: 654.471439\n",
      "Epoch 99, loss: 639.655512\n",
      "Epoch 100, loss: 656.200734\n",
      "Epoch 101, loss: 654.854792\n",
      "Epoch 102, loss: 657.418259\n",
      "Epoch 103, loss: 656.793782\n",
      "Epoch 104, loss: 651.550833\n",
      "Epoch 105, loss: 647.663730\n",
      "Epoch 106, loss: 653.229900\n",
      "Epoch 107, loss: 646.843268\n",
      "Epoch 108, loss: 653.577158\n",
      "Epoch 109, loss: 660.947737\n",
      "Epoch 110, loss: 667.644646\n",
      "Epoch 111, loss: 650.912202\n",
      "Epoch 112, loss: 636.406865\n",
      "Epoch 113, loss: 649.376745\n",
      "Epoch 114, loss: 653.012898\n",
      "Epoch 115, loss: 641.253332\n",
      "Epoch 116, loss: 654.138872\n",
      "Epoch 117, loss: 644.179307\n",
      "Epoch 118, loss: 665.492107\n",
      "Epoch 119, loss: 648.941719\n",
      "Epoch 120, loss: 651.196939\n",
      "Epoch 121, loss: 651.155917\n",
      "Epoch 122, loss: 652.971077\n",
      "Epoch 123, loss: 647.497691\n",
      "Epoch 124, loss: 648.007659\n",
      "Epoch 125, loss: 654.185781\n",
      "Epoch 126, loss: 661.452207\n",
      "Epoch 127, loss: 651.048162\n",
      "Epoch 128, loss: 650.863551\n",
      "Epoch 129, loss: 660.665579\n",
      "Epoch 130, loss: 646.024427\n",
      "Epoch 131, loss: 659.978918\n",
      "Epoch 132, loss: 652.052251\n",
      "Epoch 133, loss: 650.086309\n",
      "Epoch 134, loss: 644.622381\n",
      "Epoch 135, loss: 645.334031\n",
      "Epoch 136, loss: 655.462821\n",
      "Epoch 137, loss: 666.586587\n",
      "Epoch 138, loss: 650.856973\n",
      "Epoch 139, loss: 648.315232\n",
      "Epoch 140, loss: 654.755672\n",
      "Epoch 141, loss: 647.641665\n",
      "Epoch 142, loss: 649.969590\n",
      "Epoch 143, loss: 646.451922\n",
      "Epoch 144, loss: 650.742267\n",
      "Epoch 145, loss: 643.200565\n",
      "Epoch 146, loss: 653.915031\n",
      "Epoch 147, loss: 647.129931\n",
      "Epoch 148, loss: 639.144273\n",
      "Epoch 149, loss: 656.881927\n",
      "Epoch 150, loss: 637.541819\n",
      "Epoch 151, loss: 648.961818\n",
      "Epoch 152, loss: 651.262401\n",
      "Epoch 153, loss: 649.196359\n",
      "Epoch 154, loss: 643.465053\n",
      "Epoch 155, loss: 655.236063\n",
      "Epoch 156, loss: 644.213480\n",
      "Epoch 157, loss: 653.554980\n",
      "Epoch 158, loss: 652.736672\n",
      "Epoch 159, loss: 642.280804\n",
      "Epoch 160, loss: 632.492812\n",
      "Epoch 161, loss: 648.451733\n",
      "Epoch 162, loss: 656.792742\n",
      "Epoch 163, loss: 655.086139\n",
      "Epoch 164, loss: 644.224511\n",
      "Epoch 165, loss: 633.237182\n",
      "Epoch 166, loss: 656.510507\n",
      "Epoch 167, loss: 639.438424\n",
      "Epoch 168, loss: 641.562917\n",
      "Epoch 169, loss: 646.736507\n",
      "Epoch 170, loss: 652.656294\n",
      "Epoch 171, loss: 638.466614\n",
      "Epoch 172, loss: 642.948013\n",
      "Epoch 173, loss: 641.718652\n",
      "Epoch 174, loss: 650.704549\n",
      "Epoch 175, loss: 652.302682\n",
      "Epoch 176, loss: 643.301035\n",
      "Epoch 177, loss: 654.093542\n",
      "Epoch 178, loss: 634.169996\n",
      "Epoch 179, loss: 648.663685\n",
      "Epoch 180, loss: 649.906555\n",
      "Epoch 181, loss: 650.699419\n",
      "Epoch 182, loss: 652.466021\n",
      "Epoch 183, loss: 634.093270\n",
      "Epoch 184, loss: 665.640933\n",
      "Epoch 185, loss: 644.218595\n",
      "Epoch 186, loss: 639.021225\n",
      "Epoch 187, loss: 641.265069\n",
      "Epoch 188, loss: 638.017685\n",
      "Epoch 189, loss: 653.521684\n",
      "Epoch 190, loss: 636.439690\n",
      "Epoch 191, loss: 646.587058\n",
      "Epoch 192, loss: 651.498376\n",
      "Epoch 193, loss: 646.197800\n",
      "Epoch 194, loss: 640.614318\n",
      "Epoch 195, loss: 642.158201\n",
      "Epoch 196, loss: 648.862349\n",
      "Epoch 197, loss: 646.685774\n",
      "Epoch 198, loss: 667.492373\n",
      "Epoch 199, loss: 640.749068\n",
      "lr: 1e-05 | reg: 1e-05 | accuracy: 0.208\n",
      "Epoch 0, loss: 690.829360\n",
      "Epoch 1, loss: 689.090449\n",
      "Epoch 2, loss: 689.193265\n",
      "Epoch 3, loss: 688.404256\n",
      "Epoch 4, loss: 686.820982\n",
      "Epoch 5, loss: 686.439156\n",
      "Epoch 6, loss: 683.432193\n",
      "Epoch 7, loss: 684.530250\n",
      "Epoch 8, loss: 684.889939\n",
      "Epoch 9, loss: 682.828742\n",
      "Epoch 10, loss: 681.758145\n",
      "Epoch 11, loss: 684.821233\n",
      "Epoch 12, loss: 680.730858\n",
      "Epoch 13, loss: 680.078673\n",
      "Epoch 14, loss: 680.578255\n",
      "Epoch 15, loss: 679.758442\n",
      "Epoch 16, loss: 681.152660\n",
      "Epoch 17, loss: 678.929300\n",
      "Epoch 18, loss: 677.125794\n",
      "Epoch 19, loss: 678.129138\n",
      "Epoch 20, loss: 674.775579\n",
      "Epoch 21, loss: 676.556487\n",
      "Epoch 22, loss: 674.957792\n",
      "Epoch 23, loss: 680.953821\n",
      "Epoch 24, loss: 676.774317\n",
      "Epoch 25, loss: 673.479203\n",
      "Epoch 26, loss: 677.109552\n",
      "Epoch 27, loss: 674.564617\n",
      "Epoch 28, loss: 673.977588\n",
      "Epoch 29, loss: 669.051264\n",
      "Epoch 30, loss: 671.332264\n",
      "Epoch 31, loss: 676.039025\n",
      "Epoch 32, loss: 668.319804\n",
      "Epoch 33, loss: 672.002073\n",
      "Epoch 34, loss: 669.694434\n",
      "Epoch 35, loss: 672.414647\n",
      "Epoch 36, loss: 669.437410\n",
      "Epoch 37, loss: 670.439228\n",
      "Epoch 38, loss: 671.912638\n",
      "Epoch 39, loss: 671.830694\n",
      "Epoch 40, loss: 669.585240\n",
      "Epoch 41, loss: 674.888506\n",
      "Epoch 42, loss: 670.298330\n",
      "Epoch 43, loss: 667.596981\n",
      "Epoch 44, loss: 665.715988\n",
      "Epoch 45, loss: 664.418255\n",
      "Epoch 46, loss: 656.723969\n",
      "Epoch 47, loss: 663.393417\n",
      "Epoch 48, loss: 671.296120\n",
      "Epoch 49, loss: 667.921123\n",
      "Epoch 50, loss: 661.472925\n",
      "Epoch 51, loss: 659.829814\n",
      "Epoch 52, loss: 666.190447\n",
      "Epoch 53, loss: 658.741415\n",
      "Epoch 54, loss: 671.923581\n",
      "Epoch 55, loss: 663.587738\n",
      "Epoch 56, loss: 668.258606\n",
      "Epoch 57, loss: 670.193559\n",
      "Epoch 58, loss: 664.803759\n",
      "Epoch 59, loss: 662.079866\n",
      "Epoch 60, loss: 666.928655\n",
      "Epoch 61, loss: 668.564910\n",
      "Epoch 62, loss: 659.887868\n",
      "Epoch 63, loss: 662.934116\n",
      "Epoch 64, loss: 660.772325\n",
      "Epoch 65, loss: 656.178042\n",
      "Epoch 66, loss: 653.210097\n",
      "Epoch 67, loss: 665.803820\n",
      "Epoch 68, loss: 663.381960\n",
      "Epoch 69, loss: 661.158968\n",
      "Epoch 70, loss: 656.218928\n",
      "Epoch 71, loss: 668.287659\n",
      "Epoch 72, loss: 645.523407\n",
      "Epoch 73, loss: 661.896183\n",
      "Epoch 74, loss: 666.811502\n",
      "Epoch 75, loss: 661.414106\n",
      "Epoch 76, loss: 650.857510\n",
      "Epoch 77, loss: 661.357795\n",
      "Epoch 78, loss: 663.368505\n",
      "Epoch 79, loss: 649.270262\n",
      "Epoch 80, loss: 658.889128\n",
      "Epoch 81, loss: 660.231830\n",
      "Epoch 82, loss: 653.139823\n",
      "Epoch 83, loss: 653.372060\n",
      "Epoch 84, loss: 661.349037\n",
      "Epoch 85, loss: 654.845588\n",
      "Epoch 86, loss: 652.084441\n",
      "Epoch 87, loss: 664.889839\n",
      "Epoch 88, loss: 650.872344\n",
      "Epoch 89, loss: 661.720048\n",
      "Epoch 90, loss: 658.967285\n",
      "Epoch 91, loss: 664.957066\n",
      "Epoch 92, loss: 658.232572\n",
      "Epoch 93, loss: 656.859551\n",
      "Epoch 94, loss: 654.575698\n",
      "Epoch 95, loss: 659.896758\n",
      "Epoch 96, loss: 654.816502\n",
      "Epoch 97, loss: 638.354276\n",
      "Epoch 98, loss: 659.724569\n",
      "Epoch 99, loss: 653.837849\n",
      "Epoch 100, loss: 647.962009\n",
      "Epoch 101, loss: 649.645933\n",
      "Epoch 102, loss: 648.136504\n",
      "Epoch 103, loss: 655.828143\n",
      "Epoch 104, loss: 662.747541\n",
      "Epoch 105, loss: 656.343547\n",
      "Epoch 106, loss: 656.504839\n",
      "Epoch 107, loss: 655.452293\n",
      "Epoch 108, loss: 653.188125\n",
      "Epoch 109, loss: 650.420748\n",
      "Epoch 110, loss: 648.576673\n",
      "Epoch 111, loss: 646.734937\n",
      "Epoch 112, loss: 655.101997\n",
      "Epoch 113, loss: 652.527743\n",
      "Epoch 114, loss: 645.333710\n",
      "Epoch 115, loss: 646.908353\n",
      "Epoch 116, loss: 649.089050\n",
      "Epoch 117, loss: 657.431048\n",
      "Epoch 118, loss: 648.653626\n",
      "Epoch 119, loss: 651.759421\n",
      "Epoch 120, loss: 651.983143\n",
      "Epoch 121, loss: 659.296181\n",
      "Epoch 122, loss: 654.278133\n",
      "Epoch 123, loss: 654.146243\n",
      "Epoch 124, loss: 648.600352\n",
      "Epoch 125, loss: 650.816509\n",
      "Epoch 126, loss: 650.347547\n",
      "Epoch 127, loss: 644.085068\n",
      "Epoch 128, loss: 651.320619\n",
      "Epoch 129, loss: 645.329484\n",
      "Epoch 130, loss: 653.288780\n",
      "Epoch 131, loss: 650.073763\n",
      "Epoch 132, loss: 649.487266\n",
      "Epoch 133, loss: 654.333400\n",
      "Epoch 134, loss: 646.235222\n",
      "Epoch 135, loss: 656.785726\n",
      "Epoch 136, loss: 661.513142\n",
      "Epoch 137, loss: 657.420615\n",
      "Epoch 138, loss: 654.037774\n",
      "Epoch 139, loss: 653.894093\n",
      "Epoch 140, loss: 647.690821\n",
      "Epoch 141, loss: 653.816483\n",
      "Epoch 142, loss: 643.427688\n",
      "Epoch 143, loss: 646.255043\n",
      "Epoch 144, loss: 655.682093\n",
      "Epoch 145, loss: 645.071082\n",
      "Epoch 146, loss: 652.400528\n",
      "Epoch 147, loss: 644.938781\n",
      "Epoch 148, loss: 639.413495\n",
      "Epoch 149, loss: 639.237300\n",
      "Epoch 150, loss: 657.197925\n",
      "Epoch 151, loss: 640.471256\n",
      "Epoch 152, loss: 647.490938\n",
      "Epoch 153, loss: 648.458664\n",
      "Epoch 154, loss: 647.288703\n",
      "Epoch 155, loss: 648.709208\n",
      "Epoch 156, loss: 651.443475\n",
      "Epoch 157, loss: 654.982078\n",
      "Epoch 158, loss: 646.002608\n",
      "Epoch 159, loss: 648.167535\n",
      "Epoch 160, loss: 653.587232\n",
      "Epoch 161, loss: 639.965981\n",
      "Epoch 162, loss: 647.199253\n",
      "Epoch 163, loss: 628.191916\n",
      "Epoch 164, loss: 650.744846\n",
      "Epoch 165, loss: 647.924119\n",
      "Epoch 166, loss: 646.477960\n",
      "Epoch 167, loss: 634.052528\n",
      "Epoch 168, loss: 644.554642\n",
      "Epoch 169, loss: 640.511715\n",
      "Epoch 170, loss: 646.031305\n",
      "Epoch 171, loss: 644.893696\n",
      "Epoch 172, loss: 644.484118\n",
      "Epoch 173, loss: 644.630947\n",
      "Epoch 174, loss: 637.282295\n",
      "Epoch 175, loss: 652.446644\n",
      "Epoch 176, loss: 644.673664\n",
      "Epoch 177, loss: 648.027648\n",
      "Epoch 178, loss: 636.774916\n",
      "Epoch 179, loss: 653.663791\n",
      "Epoch 180, loss: 635.301855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181, loss: 643.604282\n",
      "Epoch 182, loss: 638.131711\n",
      "Epoch 183, loss: 649.234109\n",
      "Epoch 184, loss: 655.660951\n",
      "Epoch 185, loss: 634.886128\n",
      "Epoch 186, loss: 639.011080\n",
      "Epoch 187, loss: 644.099267\n",
      "Epoch 188, loss: 648.632612\n",
      "Epoch 189, loss: 650.104234\n",
      "Epoch 190, loss: 637.131449\n",
      "Epoch 191, loss: 643.870670\n",
      "Epoch 192, loss: 651.536155\n",
      "Epoch 193, loss: 654.346541\n",
      "Epoch 194, loss: 631.883629\n",
      "Epoch 195, loss: 639.900779\n",
      "Epoch 196, loss: 634.201557\n",
      "Epoch 197, loss: 653.656472\n",
      "Epoch 198, loss: 645.853348\n",
      "Epoch 199, loss: 653.357150\n",
      "lr: 1e-05 | reg: 1e-06 | accuracy: 0.203\n",
      "best validation accuracy achieved: 0.210000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "# train_folds_x = np.split(train_X, batch_size)\n",
    "# train_folds_y = np.split(train_y, batch_size)\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "#         sub_train_x = np.concatenate(train_folds_x[1:])\n",
    "#         sub_train_y = np.concatenate(train_folds_x[1:])\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "#         classifier.fit(sub_train_x, sub_train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        \n",
    "#         pred = classifier.predict(train_folds_x[0])\n",
    "        pred = classifier.predict(test_X)\n",
    "#         accuracy = multiclass_accuracy(pred, train_folds_y[0])\n",
    "        accuracy = multiclass_accuracy(pred, test_y)\n",
    "        \n",
    "        print(f'lr: {lr} | reg: {reg} | accuracy: {accuracy}')\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
